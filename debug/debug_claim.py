import torch
import re
import os
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from underthesea import sent_tokenize
import torch.nn.functional as F

# --- C·∫§U H√åNH ---
# S·ª≠a l·∫°i ƒë∆∞·ªùng d·∫´n model n·∫øu c·∫ßn
MODEL_PATHS = ["claim_detector_model", "model/claim_detector_model"]
MODEL_PATH = next((p for p in MODEL_PATHS if os.path.exists(p)), None)

def load_model():
    if not MODEL_PATH:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y folder model claim detector. S·∫Ω ch·ªâ ch·∫°y Heuristic.")
        return None, None
        
    print(f"‚è≥ ƒêang t·∫£i model t·ª´: {MODEL_PATH}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
    return tokenizer, model

def custom_segmentation(text):
    """T√°ch c√¢u th√¥ng minh h∆°n cho d·ªØ li·ªáu web"""
    # 1. T√°ch theo xu·ªëng d√≤ng tr∆∞·ªõc
    paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
    final_sentences = []
    for p in paragraphs:
        # N·∫øu ƒëo·∫°n vƒÉn qu√° ng·∫Øn (VD: Ti√™u ƒë·ªÅ, Ng√†y th√°ng), coi l√† 1 c√¢u
        if len(p) < 30:
            final_sentences.append(p)
        else:
            # ƒêo·∫°n d√†i th√¨ d√πng underthesea t√°ch
            sents = sent_tokenize(p)
            for s in sents:
                if len(s.strip()) > 5: # L·ªçc c√¢u qu√° ng·∫Øn
                    final_sentences.append(s.strip())
    return final_sentences

def check_heuristic(text):
    """Lu·∫≠t c∆° b·∫£n: C√≥ s·ªë li·ªáu ho·∫∑c th·ª±c th·ªÉ vi·∫øt hoa"""
    has_digit = bool(re.search(r'\d+', text))
    has_cap = bool(re.search(r'[A-Zƒê][a-z√†-·ªπ]+', text))
    
    # L·ªçc r√°c qu·∫£ng c√°o
    is_spam = bool(re.search(r'(li√™n h·ªá|qu·∫£ng c√°o|b·∫£n quy·ªÅn|·∫£nh:|ngu·ªìn:)', text.lower()))
    
    if is_spam: return False, "Spam"
    if has_digit: return True, "Has Digit"
    if has_cap and len(text.split()) > 10: return True, "Has Entity"
    
    return False, "No Signal"

def debug_text(title, text, tokenizer, model):
    print("\n" + "#"*70)
    print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è DEBUG B√ÄI B√ÅO: {title}")
    print("#"*70)
    
    # 1. T√°ch c√¢u
    sentences = custom_segmentation(text)
    print(f"üìä ƒê√£ t√°ch th√†nh {len(sentences)} c√¢u.")
    print("-" * 105)
    print(f"{'C√ÇU (C·∫Øt g·ªçn)':<50} | {'MODEL':<10} | {'LU·∫¨T (Rule)':<15} | {'QUY·∫æT ƒê·ªäNH'}")
    print("-" * 105)
    
    kept_claims = 0
    
    for sent in sentences:
        # Check Model AI
        model_score = 0.0
        if model:
            inputs = tokenizer(sent, return_tensors="pt", truncation=True, max_length=128)
            with torch.no_grad():
                outputs = model(**inputs)
                probs = F.softmax(outputs.logits, dim=1)
                model_score = probs[0][1].item() # X√°c su·∫•t l√† Claim (Label 1)
        
        # Check Lu·∫≠t
        is_h, h_reason = check_heuristic(sent)
        
        # Quy·∫øt ƒë·ªãnh cu·ªëi c√πng (Gi·∫£ l·∫≠p logic trong verifier)
        # L·∫•y n·∫øu: Model t·ª± tin (>0.5) HO·∫∂C Lu·∫≠t th·ªèa m√£n
        final_decision = "‚ùå LO·∫†I"
        if model_score > 0.5 or is_h:
            final_decision = "‚úÖ L·∫§Y"
            kept_claims += 1
            
        # Format in ·∫•n
        sent_display = (sent[:47] + "...") if len(sent) > 47 else sent.ljust(50)
        score_display = f"{model_score:.4f}" if model else "N/A"
        
        # T√¥ m√†u (n·∫øu terminal h·ªó tr·ª£) ho·∫∑c d√πng k√Ω t·ª±
        print(f"{sent_display} | {score_display:<10} | {h_reason:<15} | {final_decision}")
        
    print("-" * 105)
    print(f"üëâ T·ªîNG K·∫æT: Gi·ªØ l·∫°i {kept_claims}/{len(sentences)} c√¢u l√†m b·∫±ng ch·ª©ng.")

if __name__ == "__main__":
    tokenizer, model = load_model()
    
    # --- D·ªÆ LI·ªÜU C·ª¶A B·∫†N ---
    
    # 1. B√°o gi·∫£ (Nisha Patel)
    fake_news = """
    [G√≥c nh√¨n kh√°c] Bi k·ªãch c·ªßa n·ªØ c·∫£nh s√°t y√™u l·∫ßm
    Anh Nisha Patel-Nasri v·∫•t v·∫£ l√†m vi·ªác ƒë·ªÉ mua nh√†, c·∫•p v·ªën cho ch·ªìng kinh doanh m√† kh√¥ng hay bi·∫øt anh ta phung ph√≠ ti·ªÅn b·∫°c cho ng∆∞·ªùi t√¨nh b√≠ m·∫≠t l√† g√°i b√°n d√¢m.
    g·∫ßn n·ª≠a ƒë√™m 13/5 / 2006, h√†ng x√≥m trong khu ph·ªë ·ªü wembley, london, b·ªóng nghe th·∫•y ti·∫øng ph·ª• n·ªØ h√©t th·∫•t thanh.
    h·ªç ƒëi ra ngo√†i ki·ªÉm tra th√¨ th·∫•y nisha patel - nasri, 34 tu·ªïi, ƒëang √¥m v·∫øt th∆∞∆°ng ch·∫£y r·∫•t nhi·ªÅu m√°u tr√™n ƒë∆∞·ªùng l√°i xe v√†o nh√†.
    c·∫£nh s√°t cho bi·∫øt nisha b·ªã ƒë√¢m m·ªôt nh√°t duy nh·∫•t ·ªü ƒë√πi tr√°i, s√¢u 26 cm l√†m th·ªßng ƒë·ªông m·∫°ch.
    Fadi b·ªã b·∫Øt v√¨ t·ªôi gi·∫øt v·ª£ v√†o ng√†y 32/2 / 2007.
    ng√†y 56/5 / 2008, fadi, rodger v√† jason b·ªã b·ªìi th·∫©m ƒëo√†n k·∫øt t·ªôi gi·∫øt ng∆∞·ªùi.
    ti·∫øn sƒ© khoa h·ªçc l√Ω ti·∫øn nam cho bi·∫øt.
    """
    
    # 2. B√°o th·∫≠t (L√Ω H·∫£i)
    real_news = """
    L√Ω H·∫£i chi·∫øu phim mi·ªÖn ph√≠ cho 2.000 ng∆∞·ªùi d√¢n
    ƒê·ªìng Th√°p ƒê·∫°o di·ªÖn L√Ω H·∫£i chi·∫øu phim mi·ªÖn ph√≠ cho h∆°n 2.000 kh√°n gi·∫£ ·ªü x√£ ƒê·ªãnh Y√™n - b·ªëi c·∫£nh phim "L·∫≠t m·∫∑t 6", t·ªëi 24/4.
    ƒë·∫øn g·∫ßn gi·ªù chi·∫øu, s·ªë l∆∞·ª£ng ng∆∞·ªùi l√™n ƒë·∫øn g·∫ßn 4.000, √™k√≠p bu·ªôc ph·∫£i t·ª´ ch·ªëi b·ªõt do kh√¥ng s·∫Øp x·∫øp ƒë·ªß kh√¥ng gian.
    ƒëo√†n phim mua h√†ng ngh√¨n chi·∫øc chi·∫øu x·∫øp quanh l√†ng ƒë·ªÉ ghi h√¨nh.
    l√Ω h·∫£i ph·∫£i chi ti·ªÅn t·ª∑ ƒë·ªÉ t√°i hi·ªán b·ªëi c·∫£nh.
    nƒÉm 2021, l·∫≠t m·∫∑t 5: 48h c·ªßa l√Ω h·∫£i ƒë·∫°t doanh thu 150 t·ª∑ ƒë·ªìng.
    """

    debug_text("B√ÅO GI·∫¢ (NISHA PATEL)", fake_news, tokenizer, model)
    debug_text("B√ÅO TH·∫¨T (L√ù H·∫¢I)", real_news, tokenizer, model)