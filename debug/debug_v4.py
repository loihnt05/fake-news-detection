import psycopg2
import torch
import os
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.nn.functional import softmax
from dotenv import load_dotenv

load_dotenv()

# Cáº¥u hÃ¬nh
MODEL_PATH = "my_model_v4" # Äáº£m báº£o Ä‘Ãºng tÃªn folder báº¡n Ä‘Ã£ giáº£i nÃ©n
DB_CONFIG = {
    "dbname": os.getenv("POSTGRES_DB", "vnexpress_scraper"),
    "user": os.getenv("POSTGRES_USER", "admin"),
    "password": os.getenv("POSTGRES_PASSWORD", "admin"),
    "host": os.getenv("DB_HOST", "localhost"),
    "port": os.getenv("DB_PORT", "5432")
}

def debug_pipeline():
    print(f"ğŸš€ ÄANG DEBUG Há»† THá»NG Táº I: {MODEL_PATH}")
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # 1. LOAD MODEL (NATIVE)
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)
        print("âœ… Model Load OK.")
    except Exception as e:
        print(f"âŒ Lá»—i Load Model: {e}")
        return

    # 2. LOAD RETRIEVER
    retriever = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder', device=device)

    # ---------------------------------------------------------
    # TEST 1: KIá»‚M TRA MODEL (KHÃ”NG DÃ™NG DB) - Äá»‚ CHá»¨NG MINH MODEL KHÃ”N
    # ---------------------------------------------------------
    print("\n" + "="*60)
    print("ğŸ§ª TEST 1: MODEL CÃ“ 'KHÃ”N' KHÃ”NG? (Hardcoded Input)")
    print("="*60)
    
    claim_test = "V-League 2024-2025 dá»± kiáº¿n khai máº¡c vÃ o thÃ¡ng 12."
    evid_test  = "V-League 2024-2025 sáº½ khai máº¡c tá»« ngÃ y 23/8." # CÃ¢u chuáº©n ngáº¯n gá»n

    inputs = tokenizer(claim_test, evid_test, return_tensors='pt', truncation=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = softmax(outputs.logits, dim=1)[0].cpu().numpy()
    
    print(f"Claim: {claim_test}")
    print(f"Evid : {evid_test}")
    print(f"ğŸ“Š Scores: REFUTED={probs[0]:.4f} | SUPPORTED={probs[1]:.4f} | NEI={probs[2]:.4f}")
    
    if probs[0] > 0.9:
        print("ğŸ‘‰ Káº¾T QUáº¢: âœ… MODEL HOáº T Äá»˜NG Tá»T (Báº¯t Ä‘Æ°á»£c FAKE).")
    else:
        print("ğŸ‘‰ Káº¾T QUáº¢: âŒ MODEL Bá»Š Lá»–I (KhÃ´ng giá»‘ng trÃªn Colab).")

    # ---------------------------------------------------------
    # TEST 2: KIá»‚M TRA RETRIEVER (DÃ™NG DB) - XEM NÃ“ TÃŒM RA CÃI GÃŒ?
    # ---------------------------------------------------------
    print("\n" + "="*60)
    print("ğŸ§ª TEST 2: RETRIEVER TÃŒM THáº¤Y CÃI QUÃI GÃŒ? (DB Input)")
    print("="*60)
    
    # MÃ£ hÃ³a Claim
    vec = retriever.encode(claim_test)
    
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    
    # Láº¥y Top 3 cÃ¢u gáº§n nháº¥t
    cur.execute("""
        SELECT content, (embedding <=> %s::vector) as distance
        FROM sentence_store
        ORDER BY distance ASC
        LIMIT 3; 
    """, (vec.tolist(),))
    rows = cur.fetchall()
    
    print(f"ğŸ” Truy váº¥n: '{claim_test}'")
    
    found_good_evidence = False
    
    for i, (content, dist) in enumerate(rows):
        print(f"\n--- á»¨ng viÃªn #{i+1} (Dist: {dist:.4f}) ---")
        print(f"ğŸ“„ Ná»™i dung: {content}")
        
        # ÄÆ°a vÃ o Model check thá»­
        inputs = tokenizer(claim_test, content, return_tensors='pt', truncation=True).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            probs = softmax(outputs.logits, dim=1)[0].cpu().numpy()
            
        print(f"ğŸ¤– Model phÃ¡n: REFUTED={probs[0]:.2f} | NEI={probs[2]:.2f}")
        
        if probs[0] > 0.8:
            print("ğŸ‘‰ ÄÃ‚Y LÃ€ Báº°NG CHá»¨NG 'CHÃ Máº NG'! (Model báº¯t Ä‘Æ°á»£c)")
            found_good_evidence = True
        else:
            print("ğŸ‘‰ CÃ¢u nÃ y vÃ´ dá»¥ng (Model tháº¥y NEI/SUPPORTED).")

    conn.close()
    
    if not found_good_evidence:
        print("\nğŸ“¢ Káº¾T LUáº¬N: Retriever khÃ´ng tÃ¬m Ä‘Æ°á»£c cÃ¢u chá»©a '23/8' hoáº·c Distance quÃ¡ xa!")

if __name__ == "__main__":
    debug_pipeline()