import psycopg2
import torch
import os
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.nn.functional import softmax
from dotenv import load_dotenv

load_dotenv()

# --- C·∫§U H√åNH ---
# ƒê∆∞·ªùng d·∫´n ƒë·∫øn model V5 b·∫°n v·ª´a gi·∫£i n√©n
MODEL_PATH = "my_model_v5"

DB_CONFIG = {
    "dbname": os.getenv("POSTGRES_DB", "vnexpress_scraper"),
    "user": os.getenv("POSTGRES_USER", "admin"),
    "password": os.getenv("POSTGRES_PASSWORD", "admin"),
    "host": os.getenv("DB_HOST", "localhost"),
    "port": os.getenv("DB_PORT", "5432")
}

def debug_v5():
    print(f"üöÄ ƒêANG LOAD MODEL V5 T·ª™: {MODEL_PATH}")
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # 1. LOAD MODEL (NATIVE)
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)
        print("‚úÖ Model V5 Load OK (Native Mode).")
    except Exception as e:
        print(f"‚ùå L·ªói Load Model: {e}")
        print("üëâ H√£y ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c model!")
        return

    # 2. LOAD RETRIEVER
    print("‚è≥ ƒêang load Retriever...")
    retriever = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder', device=device)

    # ---------------------------------------------------------
    # TEST CASE KH√ì NH·∫§T (C√ÇU D√ÄI)
    # ---------------------------------------------------------
    print("\n" + "="*60)
    print("üß™ KI·ªÇM TRA ƒê·ªò TH√îNG MINH (Hard Case)")
    print("="*60)
    
    claim = "V-League 2024-2025 d·ª± ki·∫øn khai m·∫°c v√†o th√°ng 12. v·ªõi 24 trong t·ªïng s·ªë 26 v√≤ng ƒë·∫•u d·ª± ki·∫øn di·ªÖn ra v√†o ng√†y cu·ªëi tu·∫ßn."
    # Gi·∫£ l·∫≠p Evidence t√¨m ƒë∆∞·ª£c t·ª´ DB (C√¢u ƒë√∫ng nh∆∞ng d√†i)
    evidence_simulated = "V-League 2024-2025 s·∫Ω khai m·∫°c t·ª´ ng√†y 23/8. v·ªõi 24 trong t·ªïng s·ªë 26 v√≤ng ƒë·∫•u d·ª± ki·∫øn di·ªÖn ra v√†o ng√†y cu·ªëi tu·∫ßn."

    inputs = tokenizer(claim, evidence_simulated, return_tensors='pt', truncation=True, max_length=256).to(device)
    
    with torch.no_grad():
        outputs = model(**inputs)
        probs = softmax(outputs.logits, dim=1)[0].cpu().numpy()
    
    print(f"üîπ Claim: ...th√°ng 12... [ƒëu√¥i d√†i]")
    print(f"üî∏ Evid : ...ng√†y 23/8... [ƒëu√¥i d√†i]")
    
    labels = ["FAKE üõë", "REAL ‚úÖ", "NEI ‚ö™"]
    idx = probs.argmax()
    
    print(f"\nüìä Scores: FAKE={probs[0]:.4f} | REAL={probs[1]:.4f} | NEI={probs[2]:.4f}")
    print(f"üëâ K·∫æT QU·∫¢: {labels[idx]}")

    if idx == 0 and probs[0] > 0.8:
        print("üéâ TUY·ªÜT V·ªúI! Model ƒë√£ b·∫Øt ƒë∆∞·ª£c l·ªói trong c√¢u d√†i.")
    else:
        print("‚ö†Ô∏è Model v·∫´n c√≤n l∆∞·ª°ng l·ª±.")

    # ---------------------------------------------------------
    # TEST DB INTEGRATION
    # ---------------------------------------------------------
    print("\n" + "="*60)
    print("üì° KI·ªÇM TRA D·ªÆ LI·ªÜU T·ª™ DATABASE")
    print("="*60)
    
    vec = retriever.encode(claim)
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    
    # L·∫•y c√¢u t·ªët nh·∫•t
    cur.execute("""
        SELECT content, (embedding <=> %s::vector) as distance
        FROM sentence_store
        ORDER BY distance ASC
        LIMIT 1; 
    """, (vec.tolist(),))
    
    row = cur.fetchone()
    conn.close()
    
    if row:
        db_content, dist = row
        print(f"üîé T√¨m th·∫•y trong DB (Dist: {dist:.4f}):")
        print(f"üìÑ {db_content}")
        
        # Check th·∫≠t
        inputs = tokenizer(claim, db_content, return_tensors='pt', truncation=True).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            probs = softmax(outputs.logits, dim=1)[0].cpu().numpy()
        
        final_lbl = labels[probs.argmax()]
        print(f"\nü§ñ Model ph√°n quy·∫øt v·ªõi d·ªØ li·ªáu DB: {final_lbl} (FAKE Score: {probs[0]:.2f})")
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu n√†o trong DB.")

if __name__ == "__main__":
    debug_v5()