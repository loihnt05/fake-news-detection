import torch
import torch.nn.functional as F
import faiss
import pickle
import numpy as np
import re
import time
from pathlib import Path
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer

# ================= C·∫§U H√åNH H·ªÜ TH·ªêNG =================
# ƒê∆∞·ªùng d·∫´n file (S·ª≠a l·∫°i cho ƒë√∫ng th∆∞ m·ª•c c·ªßa b·∫°n)
BASE_DIR = Path(__file__).resolve().parent.parent  # Project root directory
FAISS_INDEX_PATH = str(BASE_DIR / 'dataset' / 'articles.index')
FAISS_META_PATH = str(BASE_DIR / 'dataset' / 'articles_metadata.pkl')
CLASSIFIER_PATH = str(BASE_DIR / 'model' / 'phobert_classifier.pth')  # Model b·∫°n v·ª´a train xong

# Ng∆∞·ª°ng quy·∫øt ƒë·ªãnh (C·∫ßn tinh ch·ªânh khi test th·ª±c t·∫ø)
THRESHOLD_SIMILARITY = 20   # N·∫øu kho·∫£ng c√°ch < 5.0 => Coi l√† t√¨m th·∫•y trong DB
THRESHOLD_CONFIDENCE = 0.90  # N·∫øu x√°c su·∫•t > 90% => M·ªõi tin model ph√¢n lo·∫°i

# Thi·∫øt b·ªã
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"‚öôÔ∏è System running on: {device}")

# ================= CLASS X·ª¨ L√ù CH√çNH =================
class FakeNewsDetector:
    def __init__(self):
        print("‚è≥ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng Hybrid...")
        
        # 1. Load PhoBERT Classifier (Model 2)
        print("   - Loading Classifier Model...")
        self.tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
        self.classifier = self._load_classifier_model()
        self.classifier.to(device)
        self.classifier.eval()

        # 2. Load FAISS & SBERT (Model 1)
        print("   - Loading FAISS Database...")
        self.vector_model = SentenceTransformer('keepitreal/vietnamese-sbert')
        self.index = faiss.read_index(FAISS_INDEX_PATH)
        with open(FAISS_META_PATH, 'rb') as f:
            self.metadata = pickle.load(f)
            
        print("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng s√†ng l·ªçc tin gi·∫£!")

    def _load_classifier_model(self):
        # ƒê·ªãnh nghƒ©a l·∫°i ki·∫øn tr√∫c model ƒë·ªÉ load weights
        import torch.nn as nn
        class PhoBertClassifier(nn.Module):
            def __init__(self):
                super(PhoBertClassifier, self).__init__()
                self.bert = AutoModel.from_pretrained("vinai/phobert-base")
                self.classifier = nn.Sequential(
                    nn.Linear(768, 512),
                    nn.ReLU(),
                    nn.Dropout(0.1),
                    nn.Linear(512, 2)
                )
            def forward(self, input_ids, attention_mask):
                outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
                cls_output = outputs.last_hidden_state[:, 0, :]
                return self.classifier(cls_output)
        
        model = PhoBertClassifier()
        # Load tr·ªçng s·ªë ƒë√£ train
        model.load_state_dict(torch.load(CLASSIFIER_PATH, map_location=device))
        return model

    def normalize_text(self, text):
        # H√†m l√†m s·∫°ch gi·ªëng h·ªát l√∫c train
        if not isinstance(text, str): return ""
        text = re.sub(r'\s+([.,;?!:])', r'\1', text) # X√≥a space th·ª´a
        text = re.sub(r'(\d)\s+/\s+(\d)', r'\1/\2', text) # Fix ng√†y th√°ng
        text = re.sub(r'^[A-Zƒê√Ä-·ª∏ ]+\s*-\s*', '', text) # X√≥a ngu·ªìn tin l·ªô
        return text.strip()

    def check(self, raw_text):
        t0 = time.time()
        clean_text = self.normalize_text(raw_text)
        
        # === B∆Ø·ªöC 1: TRA C·ª®U DATABASE (FAISS) ===
        query_vec = self.vector_model.encode([clean_text])
        D, I = self.index.search(query_vec, k=1) 
        
        distance = D[0][0]
        db_idx = I[0][0]

        # In ra ƒë·ªÉ b·∫°n tinh ch·ªânh (Sau n√†y x√≥a ƒëi)
        print(f"   [Debug] Distance: {distance:.2f}") 
        
        # === B∆Ø·ªöC 2: PH√ÇN T√çCH VƒÇN PHONG (CLASSIFIER) ===
        inputs = self.tokenizer(clean_text, return_tensors="pt", truncation=True, max_length=128, padding='max_length')
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            logits = self.classifier(inputs['input_ids'], inputs['attention_mask'])
            probs = F.softmax(logits, dim=1)
            
        fake_prob = probs[0][0].item()
        real_prob = probs[0][1].item()

        # === B∆Ø·ªöC 3: RA QUY·∫æT ƒê·ªäNH (LOGIC HYBRID M·ªöI) ===
        
        # Case A: T√¨m th·∫•y b√†i gi·ªëng h·ªát trong DB (Kho·∫£ng c√°ch r·∫•t g·∫ßn)
        if distance < THRESHOLD_SIMILARITY and db_idx != -1: # V√≠ d·ª• < 5.0
            label_code = self.metadata['labels'][db_idx]
            label = "REAL" if label_code == 1 else "FAKE"
            return {
                "result": label,
                "reason": "MATCH_DB",
                "message": f"Kh·ªõp d·ªØ li·ªáu g·ªëc (ƒê·ªô l·ªách: {distance:.2f})",
                "confidence": 1.0,
                "time": time.time() - t0
            }

        # Case B: N·ªôi dung qu√° xa l·∫° (Distance qu√° l·ªõn) -> UNDEFINED NGAY L·∫¨P T·ª®C
        # ƒê√¢y ch√≠nh l√† c√°i "l∆∞·ªõi" ƒë·ªÉ b·∫Øt c√¢u Ng∆∞·ªùi ngo√†i h√†nh tinh
        THRESHOLD_UNKNOWN = 55 # B·∫°n h√£y ch·ªânh s·ªë n√†y d·ª±a tr√™n k·∫øt qu·∫£ debug
        
        if distance > THRESHOLD_UNKNOWN:
            return {
                "result": "UNDEFINED",
                "reason": "UNKNOWN_TOPIC", # L√Ω do: Ch·ªß ƒë·ªÅ l·∫°
                "message": f"N·ªôi dung qu√° m·ªõi ho·∫∑c l·∫° l·∫´m (Distance: {distance:.2f}). AI ch∆∞a ƒë·ªß d·ªØ li·ªáu ki·ªÉm ch·ª©ng.",
                "confidence": 0.0,
                "time": time.time() - t0
            }

        # Case C: N·ªôi dung c√≥ li√™n quan (5 < Distance < 25) -> Tin v√†o Classifier
        if real_prob > THRESHOLD_CONFIDENCE:
            return {
                "result": "REAL",
                "reason": "AI_PREDICT",
                "confidence": real_prob,
                "message": f"VƒÉn phong tin c·∫≠y ({real_prob:.1%})",
                "time": time.time() - t0
            }
        elif fake_prob > THRESHOLD_CONFIDENCE:
            return {
                "result": "FAKE",
                "reason": "AI_PREDICT",
                "confidence": fake_prob,
                "message": f"VƒÉn phong l·ª´a ƒë·∫£o ({fake_prob:.1%})",
                "time": time.time() - t0
            }
        else:
            return {
                "result": "UNDEFINED",
                "reason": "UNCERTAIN",
                "message": "AI l∆∞·ª°ng l·ª±.",
                "confidence": max(real_prob, fake_prob),
                "time": time.time() - t0
            }

# ================= CH·∫†Y TH·ª¨ =================
if __name__ == "__main__":
    detector = FakeNewsDetector()
    
    # 3 Tr∆∞·ªùng h·ª£p test kinh ƒëi·ªÉn
    test_cases = [
        # Case 1: Tin th·∫≠t (Copy t·ª´ DB ho·∫∑c s·ª≠a nh·∫π)
        "B·ªô Y t·∫ø y√™u c·∫ßu c√°c ƒë·ªãa ph∆∞∆°ng ƒë·∫©y m·∫°nh ti√™m ch·ªßng v·∫Øc xin COVID-19 m≈©i nh·∫Øc l·∫°i.",
        
        # Case 2: Tin gi·∫£ vƒÉn phong l·ª´a ƒë·∫£o (Model Classifier s·∫Ω b·∫Øt)
        "S·ªêC!!!! Chia s·∫ª ngay ƒë·ªÉ nh·∫≠n ti·ªÅn t·ª´ thi·ªán. B·∫•m v√†o link b√™n d∆∞·ªõi n·∫øu kh√¥ng s·∫Ω b·ªã kh√≥a t√†i kho·∫£n vƒ©nh vi·ªÖn!!!",
        
        # Case 3: Tin b·ªãa ƒë·∫∑t nh∆∞ng vƒÉn phong nghi√™m t√∫c (Undefined)
        "Ng∆∞·ªùi ngo√†i h√†nh tinh v·ª´a h·∫° c√°nh xu·ªëng H·ªì G∆∞∆°m v√† ƒëi ƒÉn kem Tr√†ng Ti·ªÅn chi·ªÅu nay."
    ]

    print("\n" + "="*50)
    for text in test_cases:
        print(f"\nüì∞ Input: {text}")
        res = detector.check(text)
        
        # In k·∫øt qu·∫£ ƒë·∫πp
        color = "üü¢" if res['result'] == 'REAL' else "üî¥" if res['result'] == 'FAKE' else "üü°"
        print(f"{color} K·∫æT LU·∫¨N: {res['result']}")
        print(f"   Logic: {res['reason']}")
        print(f"   Chi ti·∫øt: {res['message']}")
        if 'evidence' in res:
             print(f"   B·∫±ng ch·ª©ng: {res['evidence']}")
        print(f"   Th·ªùi gian x·ª≠ l√Ω: {res['time']:.4f}s")
    print("\n" + "="*50)