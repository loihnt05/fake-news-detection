"""
Retrain Pipeline cho Fact-Check AI
ƒê∆∞·ª£c g·ªçi b·ªüi Airflow DAG h√†ng tu·∫ßn

Pipeline:
1. L·∫•y d·ªØ li·ªáu training m·ªõi t·ª´ user feedback ƒë√£ ƒë∆∞·ª£c admin duy·ªát
2. Chu·∫©n b·ªã d·ªØ li·ªáu theo format NLI (claim, evidence, label)
3. Fine-tune Cross-Encoder model
4. Evaluate v√† l∆∞u metrics
5. L∆∞u model m·ªõi

Usage:
    python model/retrain_pipeline.py
"""

import os
import sys
import json
import torch
import psycopg2
import numpy as np
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

load_dotenv()

from sentence_transformers import CrossEncoder, InputExample
from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# --- CONFIGURATION ---
DB_CONFIG = {
    "dbname": os.getenv("POSTGRES_DB", "vnexpress_scraper"),
    "user": os.getenv("POSTGRES_USER", "admin"),
    "password": os.getenv("POSTGRES_PASSWORD", "admin"),
    "host": os.getenv("POSTGRES_HOST", "localhost"),
    "port": os.getenv("POSTGRES_PORT", "5432")
}

# Model paths
BASE_MODEL_PATH = os.getenv("BASE_MODEL_PATH", "my_model_v7")
OUTPUT_DIR = Path("model/retrained_models")
MIN_TRAINING_SAMPLES = 50  # S·ªë l∆∞·ª£ng t·ªëi thi·ªÉu ƒë·ªÉ retrain

# Training hyperparameters
TRAIN_BATCH_SIZE = 16
NUM_EPOCHS = 3
LEARNING_RATE = 2e-5
WARMUP_STEPS = 100

class RetrainPipeline:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.conn = None
        self.new_version = None
        
    def connect_db(self):
        """K·∫øt n·ªëi database"""
        self.conn = psycopg2.connect(**DB_CONFIG)
        print(f"‚úÖ Connected to DB: {DB_CONFIG['dbname']}")
        
    def get_new_training_data(self):
        """
        L·∫•y d·ªØ li·ªáu training m·ªõi t·ª´:
        1. User reports ƒë√£ ƒë∆∞·ª£c admin APPROVED
        2. Ch∆∞a ƒë∆∞·ª£c d√πng ƒë·ªÉ train version n√†o
        """
        with self.conn.cursor() as cur:
            cur.execute("""
                SELECT 
                    td.id,
                    td.claim_text,
                    td.evidence_text,
                    td.label
                FROM training_data td
                WHERE td.used_in_version IS NULL
                ORDER BY td.created_at ASC
            """)
            rows = cur.fetchall()
            
        data = []
        for row in rows:
            # Convert NLI label to numeric
            label_map = {
                'ENTAILMENT': 1,    # SUPPORTED
                'CONTRADICTION': 0, # REFUTED
                'NEUTRAL': 2        # NEI
            }
            
            data.append({
                'id': row[0],
                'claim': row[1],
                'evidence': row[2],
                'label': label_map.get(row[3], 2)
            })
            
        return data
    
    def prepare_approved_reports(self):
        """
        Chuy·ªÉn ƒë·ªïi approved reports th√†nh training data
        Logic:
        - User b√°o FAKE + AI n√≥i REAL ‚Üí CONTRADICTION
        - User b√°o REAL + AI n√≥i FAKE ‚Üí ENTAILMENT
        """
        with self.conn.cursor() as cur:
            # L·∫•y reports ƒë√£ approved ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω
            cur.execute("""
                SELECT 
                    r.id,
                    c.content as claim_text,
                    r.user_feedback,
                    r.ai_label_at_report
                FROM user_reports r
                JOIN claims c ON r.claim_id = c.id
                WHERE r.status = 'APPROVED'
                AND r.id NOT IN (SELECT report_id FROM training_data WHERE report_id IS NOT NULL)
            """)
            
            rows = cur.fetchall()
            new_samples = 0
            
            for row in rows:
                report_id, claim_text, user_feedback, ai_label = row
                
                # X√°c ƒë·ªãnh label d·ª±a tr√™n s·ª± kh√°c bi·ªát gi·ªØa user v√† AI
                if user_feedback == 'FAKE':
                    nli_label = 'CONTRADICTION'  # User ph·∫£n b√°c
                elif user_feedback == 'REAL':
                    nli_label = 'ENTAILMENT'     # User x√°c nh·∫≠n
                else:
                    nli_label = 'NEUTRAL'        # Kh√¥ng ch·∫Øc ch·∫Øn
                    
                # Th√™m v√†o training_data
                cur.execute("""
                    INSERT INTO training_data (claim_text, evidence_text, label, source, report_id)
                    VALUES (%s, %s, %s, 'user_feedback', %s)
                """, (claim_text, claim_text, nli_label, report_id))  # evidence = claim cho ƒë∆°n gi·∫£n
                new_samples += 1
                
            self.conn.commit()
            print(f"üì• ƒê√£ th√™m {new_samples} samples t·ª´ approved reports")
            
    def train_model(self, train_data):
        """Fine-tune CrossEncoder v·ªõi d·ªØ li·ªáu m·ªõi"""
        
        if len(train_data) < MIN_TRAINING_SAMPLES:
            print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ train ({len(train_data)} < {MIN_TRAINING_SAMPLES})")
            return None
            
        print(f"\nüöÄ B·∫Øt ƒë·∫ßu training v·ªõi {len(train_data)} samples...")
        
        # Prepare InputExamples
        examples = [
            InputExample(
                texts=[d['claim'], d['evidence']],
                label=float(d['label'])
            )
            for d in train_data
        ]
        
        # Split train/val
        train_examples, val_examples = train_test_split(examples, test_size=0.2, random_state=42)
        
        # Load base model
        print(f"   ‚îú‚îÄ Loading base model: {BASE_MODEL_PATH}")
        model = CrossEncoder(BASE_MODEL_PATH, num_labels=3, device=self.device)
        
        # DataLoader
        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=TRAIN_BATCH_SIZE)
        
        # Evaluator
        evaluator = None
        if len(val_examples) > 0:
            val_sentences1 = [e.texts[0] for e in val_examples]
            val_sentences2 = [e.texts[1] for e in val_examples]
            val_labels = [int(e.label) for e in val_examples]
            
        # Generate new version name
        self.new_version = f"v{datetime.now().strftime('%Y%m%d_%H%M')}"
        output_path = OUTPUT_DIR / self.new_version
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Train
        print(f"   ‚îú‚îÄ Training for {NUM_EPOCHS} epochs...")
        model.fit(
            train_dataloader=train_dataloader,
            epochs=NUM_EPOCHS,
            warmup_steps=WARMUP_STEPS,
            output_path=str(output_path),
            show_progress_bar=True
        )
        
        print(f"   ‚îî‚îÄ Model saved to: {output_path}")
        
        # Evaluate
        accuracy = self._evaluate_model(model, val_examples)
        
        return {
            'version': self.new_version,
            'path': str(output_path),
            'accuracy': accuracy,
            'training_samples': len(train_data)
        }
        
    def _evaluate_model(self, model, val_examples):
        """ƒê√°nh gi√° model tr√™n validation set"""
        if not val_examples:
            return 0.0
            
        correct = 0
        for ex in val_examples:
            pred = model.predict([ex.texts])
            pred_label = np.argmax(pred)
            if pred_label == int(ex.label):
                correct += 1
                
        accuracy = correct / len(val_examples)
        print(f"\nüìä Validation Accuracy: {accuracy:.2%}")
        return accuracy
        
    def save_version_info(self, result):
        """L∆∞u th√¥ng tin version m·ªõi v√†o DB"""
        with self.conn.cursor() as cur:
            cur.execute("""
                INSERT INTO model_versions (version, model_path, accuracy, training_samples)
                VALUES (%s, %s, %s, %s)
            """, (result['version'], result['path'], result['accuracy'], result['training_samples']))
            
            # ƒê√°nh d·∫•u training_data ƒë√£ d√πng
            cur.execute("""
                UPDATE training_data 
                SET used_in_version = %s 
                WHERE used_in_version IS NULL
            """, (result['version'],))
            
        self.conn.commit()
        print(f"‚úÖ ƒê√£ l∆∞u version info: {result['version']}")
        
    def run(self):
        """Main pipeline"""
        print("=" * 60)
        print("üîÑ RETRAIN PIPELINE STARTED")
        print("=" * 60)
        
        try:
            # 1. Connect DB
            self.connect_db()
            
            # 2. Chuy·ªÉn approved reports th√†nh training data
            print("\nüìã B∆∞·ªõc 1: X·ª≠ l√Ω approved reports...")
            self.prepare_approved_reports()
            
            # 3. L·∫•y training data m·ªõi
            print("\nüìã B∆∞·ªõc 2: L·∫•y training data...")
            train_data = self.get_new_training_data()
            print(f"   T√¨m th·∫•y {len(train_data)} samples m·ªõi")
            
            if len(train_data) < MIN_TRAINING_SAMPLES:
                print(f"\n‚è≠Ô∏è B·ªè qua training: Ch∆∞a ƒë·ªß {MIN_TRAINING_SAMPLES} samples")
                return False
                
            # 4. Train model
            print("\nüìã B∆∞·ªõc 3: Training model...")
            result = self.train_model(train_data)
            
            if result:
                # 5. Save version info
                print("\nüìã B∆∞·ªõc 4: L∆∞u th√¥ng tin version...")
                self.save_version_info(result)
                
                print("\n" + "=" * 60)
                print("‚úÖ RETRAIN PIPELINE COMPLETED SUCCESSFULLY")
                print(f"   New Version: {result['version']}")
                print(f"   Accuracy: {result['accuracy']:.2%}")
                print("=" * 60)
                return True
            else:
                print("\n‚ö†Ô∏è Training skipped")
                return False
                
        except Exception as e:
            print(f"\n‚ùå Pipeline Error: {e}")
            raise
        finally:
            if self.conn:
                self.conn.close()


if __name__ == "__main__":
    pipeline = RetrainPipeline()
    success = pipeline.run()
    sys.exit(0 if success else 1)
