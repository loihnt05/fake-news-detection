import pandas as pd
import psycopg2
import re
from simpletransformers.classification import ClassificationModel
from sklearn.model_selection import train_test_split
from underthesea import sent_tokenize
import os
from dotenv import load_dotenv
from tqdm import tqdm
import torch

load_dotenv()

DB_CONFIG = {
    "dbname": os.getenv("POSTGRES_DB", "vnexpress_scraper"),
    "user": os.getenv("POSTGRES_USER", "admin"),
    "password": os.getenv("POSTGRES_PASSWORD", "admin"),
    "host": os.getenv("DB_HOST", "localhost"),
    "port": os.getenv("DB_PORT", "5432")
}

def generate_training_data():
    print("üõ†Ô∏è ƒêang t·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán t·ª´ Database...")
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    
    # L·∫•y 5000 b√†i b·∫•t k·ª≥ ƒë·ªÉ sinh d·ªØ li·ªáu train
    cur.execute("SELECT content FROM articles LIMIT 5000")
    articles = cur.fetchall()
    
    claims = []
    non_claims = []
    
    print("‚öôÔ∏è ƒêang ph√¢n lo·∫°i d·ªØ li·ªáu m·∫´u (Heuristic)...")
    for doc in tqdm(articles):
        if not doc[0]: continue
        sentences = sent_tokenize(doc[0])
        
        for s in sentences:
            s_clean = s.strip()
            words = s_clean.split()
            
            # --- LU·∫¨T ƒê·ªÇ T·∫†O D·ªÆ LI·ªÜU M·∫™U (CH·ªà D√ôNG ƒê·ªÇ TRAIN) ---
            
            # 1. NON-CLAIM (R√°c, c√¢u d·∫´n, c√¢u h·ªèi)
            if (len(words) < 6 or 
                "?" in s_clean or 
                s_clean.lower().startswith("tuy nhi√™n") or
                s_clean.lower().startswith("theo ƒë√≥") or
                not re.search(r'[a-zA-Zƒëƒê]', s_clean)): # Kh√¥ng c√≥ ch·ªØ c√°i
                non_claims.append([s_clean, 0])
                
            # 2. CLAIM (Ch·ª©a s·ªë li·ªáu HO·∫∂C Th·ª±c th·ªÉ vi·∫øt hoa + ƒê·ªô d√†i ƒë·ªß)
            elif (re.search(r'\d+', s_clean) or re.search(r'[A-Zƒê][a-z√†-·ªπ]+', s_clean)):
                if 10 <= len(words) <= 60: # Claim th∆∞·ªùng kh√¥ng qu√° ng·∫Øn c≈©ng kh√¥ng qu√° d√†i (c·∫£ ƒëo·∫°n vƒÉn)
                    claims.append([s_clean, 1])

    # C√¢n b·∫±ng d·ªØ li·ªáu: L·∫•y 5000 Claim + 5000 Non-Claim
    min_len = min(len(claims), len(non_claims), 5000)
    
    print(f"üìä T√¨m th·∫•y: {len(claims)} claims ti·ªÅm nƒÉng, {len(non_claims)} non-claims.")
    print(f"‚öñÔ∏è ƒêang c√¢n b·∫±ng d·ªØ li·ªáu v·ªÅ {min_len} m·∫´u m·ªói lo·∫°i...")
    
    import random
    random.shuffle(claims)
    random.shuffle(non_claims)
    
    final_data = claims[:min_len] + non_claims[:min_len]
    df = pd.DataFrame(final_data, columns=["text", "labels"])
    
    # Tr·ªôn ƒë·ªÅu
    df = df.sample(frac=1).reset_index(drop=True)
    return df

def train_model():
    # 1. Chu·∫©n b·ªã d·ªØ li·ªáu
    train_df = generate_training_data()
    
    # 2. C·∫•u h√¨nh Model PhoBERT
    model_args = {
        "num_train_epochs": 2,              # Train nhanh 2 v√≤ng l√† ƒë·ªß h·ªçc pattern
        "train_batch_size": 32,
        "overwrite_output_dir": True,
        "save_model_every_epoch": False,
        "save_eval_checkpoints": False,
        "output_dir": "claim_detector_model",
        "use_multiprocessing": False,
        "fp16": torch.cuda.is_available(),
    }
    
    # 3. Kh·ªüi t·∫°o Model
    print("üöÄ ƒêang load PhoBERT base...")
    model = ClassificationModel(
        "roberta", 
        "vinai/phobert-base-v2", 
        num_labels=2, 
        args=model_args, 
        use_cuda=torch.cuda.is_available()
    )
    
    # 4. Train
    print("üî• B·∫ÆT ƒê·∫¶U TRAINING CLAIM DETECTOR...")
    train_split, eval_split = train_test_split(train_df, test_size=0.1)
    model.train_model(train_split)
    
    # 5. ƒê√°nh gi√°
    result, _, _ = model.eval_model(eval_split)
    print(f"‚úÖ K·∫øt qu·∫£ ƒë√°nh gi√°: {result}")
    print("üíæ Model ƒë√£ l∆∞u t·∫°i: ./claim_detector_model")

if __name__ == "__main__":
    # C√†i th∆∞ vi·ªán n·∫øu thi·∫øu: pip install simpletransformers
    train_model()