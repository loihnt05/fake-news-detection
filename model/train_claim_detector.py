import pandas as pd
import psycopg2
import re
import torch
import os
from sklearn.model_selection import train_test_split
from underthesea import sent_tokenize
from dotenv import load_dotenv
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset

load_dotenv()

DB_CONFIG = {
    "dbname": os.getenv("POSTGRES_DB", "vnexpress_scraper"),
    "user": os.getenv("POSTGRES_USER", "admin"),
    "password": os.getenv("POSTGRES_PASSWORD", "admin"),
    "host": os.getenv("DB_HOST", "localhost"),
    "port": os.getenv("DB_PORT", "5432")
}

# --- 1. SINH D·ªÆ LI·ªÜU (Heuristic Weak Supervision) ---
def generate_training_data():
    print("üõ†Ô∏è ƒêang t·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán t·ª´ DB...")
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    
    # L·∫•y 5000 b√†i ƒë·ªÉ l√†m m·∫´u
    cur.execute("SELECT content FROM articles LIMIT 5000")
    articles = cur.fetchall()
    
    claims = []
    non_claims = []
    
    print("‚öôÔ∏è ƒêang ph√¢n lo·∫°i d·ªØ li·ªáu m·∫´u...")
    for doc in tqdm(articles):
        if not doc[0]: continue
        sentences = sent_tokenize(doc[0])
        for s in sentences:
            s_clean = s.strip()
            words = s_clean.split()
            
            # Label 0: Non-claim (R√°c, c√¢u h·ªèi, c√¢u d·∫´n)
            if (len(words) < 6 or "?" in s_clean or 
                s_clean.lower().startswith("tuy nhi√™n") or 
                s_clean.lower().startswith("theo ƒë√≥") or
                not re.search(r'[a-zA-Zƒëƒê]', s_clean)):
                non_claims.append({"text": s_clean, "label": 0})
            
            # Label 1: Claim (S·ªë li·ªáu, Th·ª±c th·ªÉ)
            elif (re.search(r'\d+', s_clean) or re.search(r'[A-Zƒê][a-z√†-·ªπ]+', s_clean)):
                if 10 <= len(words) <= 60:
                    claims.append({"text": s_clean, "label": 1})
    
    # C√¢n b·∫±ng d·ªØ li·ªáu
    import random
    random.shuffle(claims)
    random.shuffle(non_claims)
    min_len = min(len(claims), len(non_claims), 5000) # L·∫•y t·ªëi ƒëa 5000 m·ªói lo·∫°i
    
    final_data = claims[:min_len] + non_claims[:min_len]
    df = pd.DataFrame(final_data)
    df = df.sample(frac=1).reset_index(drop=True) # Tr·ªôn ƒë·ªÅu
    
    print(f"‚úÖ ƒê√£ t·∫°o {len(df)} m·∫´u d·ªØ li·ªáu (C√¢n b·∫±ng Claim/Non-Claim).")
    return df

# --- 2. TRAIN MODEL (HuggingFace Native) ---
def train_model():
    # A. Chu·∫©n b·ªã d·ªØ li·ªáu
    df = generate_training_data()
    
    # Chuy·ªÉn sang format Dataset c·ªßa HuggingFace
    dataset = Dataset.from_pandas(df)
    dataset = dataset.train_test_split(test_size=0.1) # Chia train/test
    
    # B. Load Tokenizer & Model
    model_name = "vinai/phobert-base-v2"
    print(f"üöÄ Loading Tokenizer & Model: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
    
    # H√†m tokenize d·ªØ li·ªáu
    def tokenize_function(examples):
        return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)
    
    print("‚öôÔ∏è Tokenizing data...")
    tokenized_datasets = dataset.map(tokenize_function, batched=True)
    
    # C. C·∫•u h√¨nh Training
    training_args = TrainingArguments(
        output_dir="./claim_detector_results",
        eval_strategy="epoch",  # ƒê√°nh gi√° sau m·ªói epoch
        save_strategy="no",     # Kh√¥ng l∆∞u checkpoint r√°c t·ªën dung l∆∞·ª£ng
        learning_rate=2e-5,
        per_device_train_batch_size=16, # An to√†n cho GPU 
        per_device_eval_batch_size=16,
        num_train_epochs=2,     # Train 2 v√≤ng l√† ƒë·ªß h·ªçc pattern
        weight_decay=0.01,
        use_cpu=not torch.cuda.is_available(),
        report_to="none"        # T·∫Øt wandb ƒë·ª° phi·ªÅn
    )
    
    # D. Kh·ªüi t·∫°o Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
    )
    
    # E. B·∫ÆT ƒê·∫¶U TRAIN
    print("üî• B·∫ÆT ƒê·∫¶U TRAINING (Native Transformers)...")
    trainer.train()
    
    # F. L∆∞u Model th√†nh ph·∫©m
    output_path = "./claim_detector_model"
    print(f"üíæ ƒêang l∆∞u model xu·ªëng '{output_path}'...")
    model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)
    print("‚úÖ HO√ÄN T·∫§T! Model ƒë√£ s·∫µn s√†ng s·ª≠ d·ª•ng.")

if __name__ == "__main__":
    train_model()