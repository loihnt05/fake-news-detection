{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083e1bf7",
   "metadata": {},
   "source": [
    "# CÃ i Ä‘áº·t thÆ° viá»‡n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60647c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[14t\u001b[1m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[35m                         \u001b[31m./\u001b[35mo\u001b[34m.\n",
      "                       \u001b[31m./\u001b[35msssso\u001b[34m-\n",
      "                     \u001b[31m`:\u001b[35mosssssss+\u001b[34m-\n",
      "                   \u001b[31m`:+\u001b[35msssssssssso\u001b[34m/.\n",
      "                 \u001b[31m`-/o\u001b[35mssssssssssssso\u001b[34m/.\n",
      "               \u001b[31m`-/+\u001b[35msssssssssssssssso\u001b[34m+:`\n",
      "             \u001b[31m`-:/+\u001b[35msssssssssssssssssso\u001b[34m+/.\n",
      "           \u001b[31m`.://o\u001b[35msssssssssssssssssssso\u001b[34m++-\n",
      "          \u001b[31m.://+\u001b[35mssssssssssssssssssssssso\u001b[34m++:\n",
      "        \u001b[31m.:///o\u001b[35mssssssssssssssssssssssssso\u001b[34m++:\n",
      "      \u001b[31m`:////\u001b[35mssssssssssssssssssssssssssso\u001b[34m+++.\n",
      "    \u001b[31m`-////+\u001b[35mssssssssssssssssssssssssssso\u001b[34m++++-\n",
      "     \u001b[31m`..-+\u001b[35moosssssssssssssssssssssssso\u001b[34m+++++/`\n",
      "       \u001b[34m./++++++++++++++++++++++++++++++/:.\n",
      "      `:::::::::::::::::::::::::------``\u001b[m\u001b[1G\u001b[20A\u001b[m\u001b[?7l\u001b[67C\n",
      "\u001b[67C\u001b[m\u001b[38;2;203;166;247mâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hardware â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\u001b[m\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;203;166;247mï„‰ PC\u001b[m\u001b[m: \u001b[m\u001b[1m\u001b[31msuperkid\u001b[m@\u001b[1m\u001b[31mloihnt-nitroan51545\u001b[m\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;203;166;247mâ”‚ â”œó°› CPU\u001b[m\u001b[m: \u001b[mAMD Ryzen 5 5600H\u001b[m\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;203;166;247mâ”‚ â”œó°› GPU\u001b[m\u001b[m: \u001b[mNVIDIA GeForce GTX 1650 Mobile / Max-Q [Discrete]\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;203;166;247mâ”‚ â”œó°› GPU\u001b[m\u001b[m: \u001b[mAMD Radeon Vega Series / Radeon Vega Mobile Series [Integrated]\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;203;166;247mâ”” â””ó°› Memory\u001b[m\u001b[m: \u001b[m11.20 GiB / 14.97 GiB (\u001b[93m75%\u001b[m)\n",
      "\u001b[67C\u001b[m\u001b[38;2;203;166;247mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\u001b[m\n",
      "\u001b[67C\n",
      "\u001b[67C\u001b[m\u001b[38;2;242;205;205mâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Software â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\u001b[m\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;242;205;205mï…¼ OS\u001b[m\u001b[m: \u001b[mEndeavourOS x86_64\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;242;205;205mâ”‚ â”œï€“ Kernel\u001b[m\u001b[m: \u001b[mLinux 6.17.9-arch1-1\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;242;205;205mâ”‚ â”œó°– Packages\u001b[m\u001b[m: \u001b[m1452 (pacman)\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;242;205;205mâ”‚ â”œï’‰ Shell\u001b[m\u001b[m: \u001b[mfish 4.2.1\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;242;205;205mâ”‚ â”œï’‰ OS Age\u001b[m\u001b[m: \u001b[m17 days\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;242;205;205mâ”” â””ï’‰ Uptime\u001b[m\u001b[m: \u001b[m6 hours, 56 mins\n",
      "\u001b[67C\u001b[m\u001b[38;2;242;205;205mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\u001b[m\n",
      "\u001b[67C\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;116;199;236mï’ˆ DE\u001b[m\u001b[m: \u001b[mKDE Plasma 6.5.3\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;116;199;236mâ”‚ â”œï’ˆ LM\u001b[m\u001b[m: \u001b[msddm 0.21.0 (Wayland)\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;116;199;236mâ”‚ â”œï’ˆ WM\u001b[m\u001b[m: \u001b[mKWin (Wayland)\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;116;199;236mâ”‚ â”œó°› GPU Driver\u001b[m\u001b[m: \u001b[mnvidia (proprietary) 580.105.08\u001b[m\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;116;199;236mâ”‚ â”œó°› GPU Driver\u001b[m\u001b[m: \u001b[mamdgpu\u001b[m\n",
      "\u001b[67C\u001b[m\u001b[1m\u001b[38;2;116;199;236mâ”” â””ó°‰¼ Theme\u001b[m\u001b[m: \u001b[mBreeze\n",
      "\u001b[67C\u001b[m\u001b[38;2;147;153;178mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\u001b[m\n",
      "\u001b[67C\u001b[m               \u001b[38mâ— \u001b[37mâ— \u001b[36mâ— \u001b[35mâ— \u001b[34mâ— \u001b[33mâ— \u001b[32mâ— \u001b[31mâ—\u001b[m\n",
      "\u001b[67C\n",
      "\u001b[?7h\u001b[2mUsing Python 3.12.12 environment at: /home/loiancut/workspace/fake-news-detection/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 21ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t\n",
    "!uv pip install transformers torch pandas scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ab26e",
   "metadata": {},
   "source": [
    "# Import vÃ  Cáº¥u hÃ¬nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74110d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loiancut/workspace/fake-news-detection/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ tÃ¬m tháº¥y GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "from tqdm.auto import tqdm # Tá»± Ä‘á»™ng chá»n thanh tiáº¿n trÃ¬nh phÃ¹ há»£p\n",
    "\n",
    "# ================= Cáº¤U HÃŒNH =================\n",
    "INPUT_FILE = '../dataset/articles_clean.csv'  # <--- Äá»•i tÃªn file náº¿u file cá»§a báº¡n khÃ¡c\n",
    "MODEL_NAME = \"vinai/phobert-base\"\n",
    "SAVE_PATH = \"phobert_classifier.pth\"\n",
    "\n",
    "# Cáº¥u hÃ¬nh Training\n",
    "MAX_LEN = 128       # Äá»™ dÃ i tá»‘i Ä‘a cá»§a cÃ¢u (128 lÃ  Ä‘á»§ cho tin tá»©c, max lÃ  256)\n",
    "BATCH_SIZE = 16     # Giáº£m xuá»‘ng 8 náº¿u bá»‹ lá»—i \"Out of Memory\" (OOM)\n",
    "EPOCHS = 3          # Sá»‘ vÃ²ng há»c\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# TÃ¹y chá»n: Chá»‰ láº¥y má»™t pháº§n dá»¯ liá»‡u Ä‘á»ƒ test code cho nhanh?\n",
    "# Set = None náº¿u muá»‘n train háº¿t 100% dá»¯ liá»‡u (Sáº½ ráº¥t lÃ¢u, khoáº£ng vÃ i tiáº¿ng)\n",
    "# Set = 5000 Ä‘á»ƒ cháº¡y thá»­ xem code cÃ³ lá»—i khÃ´ng\n",
    "SAMPLE_SIZE = None \n",
    "\n",
    "# Kiá»ƒm tra GPU\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'âœ… ÄÃ£ tÃ¬m tháº¥y GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('âš ï¸ KhÃ´ng tháº¥y GPU, sáº½ cháº¡y báº±ng CPU (Ráº¥t cháº­m!)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb48db",
   "metadata": {},
   "source": [
    "# Load & Chuáº©n bá»‹ dá»¯ liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed47cb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Äang Ä‘á»c file ../dataset/articles_clean.csv...\n",
      "   Tá»•ng sá»‘ dÃ²ng: 96239\n",
      "âœ… ÄÃ£ xÃ¡c Ä‘á»‹nh cá»™t ná»™i dung: 'content'\n",
      "âœ… ÄÃ£ xÃ¡c Ä‘á»‹nh cá»™t nhÃ£n: 'label'\n",
      "ğŸ“Š Dá»¯ liá»‡u sáºµn sÃ ng: 96239 máº«u.\n",
      "   Tá»•ng sá»‘ dÃ²ng: 96239\n",
      "âœ… ÄÃ£ xÃ¡c Ä‘á»‹nh cá»™t ná»™i dung: 'content'\n",
      "âœ… ÄÃ£ xÃ¡c Ä‘á»‹nh cá»™t nhÃ£n: 'label'\n",
      "ğŸ“Š Dá»¯ liá»‡u sáºµn sÃ ng: 96239 máº«u.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load dá»¯ liá»‡u\n",
    "print(f\"ğŸ“‚ Äang Ä‘á»c file {INPUT_FILE}...\")\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"   Tá»•ng sá»‘ dÃ²ng: {len(df)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ KhÃ´ng tÃ¬m tháº¥y file {INPUT_FILE}. HÃ£y kiá»ƒm tra láº¡i tÃªn file!\")\n",
    "    \n",
    "# 2. Tá»± Ä‘á»™ng tÃ¬m tÃªn cá»™t chá»©a ná»™i dung\n",
    "possible_text_cols = ['content', 'text', 'news', 'article', 'body']\n",
    "col_text = next((col for col in possible_text_cols if col in df.columns), None)\n",
    "\n",
    "# Tá»± Ä‘á»™ng tÃ¬m tÃªn cá»™t nhÃ£n\n",
    "possible_label_cols = ['label', 'target', 'is_fake']\n",
    "col_label = next((col for col in possible_label_cols if col in df.columns), None)\n",
    "\n",
    "if col_text and col_label:\n",
    "    print(f\"âœ… ÄÃ£ xÃ¡c Ä‘á»‹nh cá»™t ná»™i dung: '{col_text}'\")\n",
    "    print(f\"âœ… ÄÃ£ xÃ¡c Ä‘á»‹nh cá»™t nhÃ£n: '{col_label}'\")\n",
    "else:\n",
    "    raise ValueError(f\"âŒ KhÃ´ng tÃ¬m tháº¥y tÃªn cá»™t phÃ¹ há»£p. Cá»™t hiá»‡n cÃ³: {list(df.columns)}\")\n",
    "\n",
    "# 3. Láº¥y máº«u (náº¿u cáº§n)\n",
    "if SAMPLE_SIZE and SAMPLE_SIZE < len(df):\n",
    "    print(f\"âš ï¸ Äang láº¥y máº«u {SAMPLE_SIZE} dÃ²ng Ä‘á»ƒ test code...\")\n",
    "    df = df.sample(SAMPLE_SIZE, random_state=42)\n",
    "\n",
    "texts = df[col_text].values\n",
    "labels = df[col_label].values\n",
    "\n",
    "print(f\"ğŸ“Š Dá»¯ liá»‡u sáºµn sÃ ng: {len(texts)} máº«u.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f4cb7",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cfd159b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Äang táº£i Tokenizer tá»« VinAI...\n",
      "ğŸš€ Báº¯t Ä‘áº§u mÃ£ hÃ³a (Tokenizing)...\n",
      "ğŸš€ Báº¯t Ä‘áº§u mÃ£ hÃ³a (Tokenizing)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96239/96239 [03:37<00:00, 442.66it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96239/96239 [03:37<00:00, 442.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ mÃ£ hÃ³a xong!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ¤– Äang táº£i Tokenizer tá»« VinAI...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# HÃ m mÃ£ hÃ³a\n",
    "def encode_batch(data_texts, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    print(\"ğŸš€ Báº¯t Ä‘áº§u mÃ£ hÃ³a (Tokenizing)...\")\n",
    "    for text in tqdm(data_texts):\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            str(text),\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "        \n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Thá»±c hiá»‡n mÃ£ hÃ³a\n",
    "input_ids, attention_masks = encode_batch(texts, MAX_LEN)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print(\"âœ… ÄÃ£ mÃ£ hÃ³a xong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34da85",
   "metadata": {},
   "source": [
    "# Chia táº­p Train / Val & Táº¡o DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "186d1d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Train set: 81803\n",
      "ğŸ”¹ Val set:   14436\n"
     ]
    }
   ],
   "source": [
    "# Chia train (85%) - validation (15%)\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    input_ids, labels, random_state=42, test_size=0.15, stratify=labels\n",
    ")\n",
    "train_masks, val_masks, _, _ = train_test_split(\n",
    "    attention_masks, labels, random_state=42, test_size=0.15, stratify=labels\n",
    ")\n",
    "\n",
    "# Táº¡o DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"ğŸ”¹ Train set: {len(train_inputs)}\")\n",
    "print(f\"ğŸ”¹ Val set:   {len(val_inputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe6cee",
   "metadata": {},
   "source": [
    "# Äá»‹nh nghÄ©a Model PhoBERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26fcce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ khá»Ÿi táº¡o Model thÃ nh cÃ´ng.\n"
     ]
    }
   ],
   "source": [
    "class PhoBertClassifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(PhoBertClassifier, self).__init__()\n",
    "        # Load PhoBERT gá»‘c\n",
    "        self.bert = AutoModel.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        # Náº¿u muá»‘n train nhanh hÆ¡n thÃ¬ Ä‘Ã³ng bÄƒng BERT (freeze_bert=True)\n",
    "        # NhÆ°ng Ä‘á»ƒ Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t thÃ¬ nÃªn Ä‘á»ƒ False (Fine-tune toÃ n bá»™)\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Lá»›p phÃ¢n loáº¡i tÃ¹y chá»‰nh\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 512), # 768 lÃ  hidden size cá»§a PhoBERT base\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 2)    # Output 2 lá»›p: Fake (0) vÃ  Real (1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward qua BERT\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Láº¥y vector [CLS] Ä‘áº¡i diá»‡n cho cáº£ cÃ¢u\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Forward qua lá»›p phÃ¢n loáº¡i\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "# Khá»Ÿi táº¡o model\n",
    "model = PhoBertClassifier(freeze_bert=False)\n",
    "model.to(device) # ÄÆ°a model vÃ o GPU\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"âœ… ÄÃ£ khá»Ÿi táº¡o Model thÃ nh cÃ´ng.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef1e3f4",
   "metadata": {},
   "source": [
    "# Báº¯t Ä‘áº§u Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272392c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Báº®T Äáº¦U TRAINING...\n",
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/5113 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/5113 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m     scheduler.step()\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# Cáº­p nháº­t loss lÃªn thanh tiáº¿n trÃ¬nh\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     progress_bar.set_postfix({\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m})\n\u001b[32m     51\u001b[39m avg_train_loss = total_train_loss / \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  âœ… Average training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# HÃ m tÃ­nh Ä‘á»™ chÃ­nh xÃ¡c\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Biáº¿n lÆ°u lá»‹ch sá»­ Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“\n",
    "training_stats = []\n",
    "\n",
    "print(\"ğŸš€ Báº®T Äáº¦U TRAINING...\")\n",
    "\n",
    "for epoch_i in range(0, EPOCHS):\n",
    "    print(f'\\n======== Epoch {epoch_i + 1} / {EPOCHS} ========')\n",
    "    print('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch_i+1}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        logits = model(b_input_ids, b_input_mask)\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_time = time.time() - t0\n",
    "    \n",
    "    # ================= VALIDATION =================\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    \n",
    "    # LÆ°u káº¿t quáº£ Ä‘á»ƒ váº½ Confusion Matrix sau nÃ y\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_input_mask)\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            \n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # LÆ°u láº¡i dá»± Ä‘oÃ¡n\n",
    "        val_preds.extend(np.argmax(logits, axis=1).flatten())\n",
    "        val_true.extend(label_ids.flatten())\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "    val_time = time.time() - t0\n",
    "    \n",
    "    print(f\"  âœ… Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  âœ… Avg Val Loss:   {avg_val_loss:.4f}\")\n",
    "    print(f\"  âœ… Val Accuracy:   {avg_val_accuracy:.4f}\")\n",
    "\n",
    "    # LÆ¯U Láº I Lá»ŠCH Sá»¬ (QUAN TRá»ŒNG)\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': val_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\\nğŸ TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa914057",
   "metadata": {},
   "source": [
    "# Loss & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77278917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL Má»šI: Váº¼ BIá»‚U Äá»’ ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Táº¡o DataFrame tá»« lá»‹ch sá»­ training\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Sá»­ dá»¥ng style seaborn cho Ä‘áº¹p\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# TÄƒng kÃ­ch thÆ°á»›c phÃ´ng chá»¯ vÃ  hÃ¬nh\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Váº½ biá»ƒu Ä‘á»“ 1: Learning Curve (Loss)\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([i for i in range(len(df_stats))], df_stats['epoch']) # Set trá»¥c x lÃ  sá»‘ epoch\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Váº½ biá»ƒu Ä‘á»“ 2: Accuracy\n",
    "plt.figure()\n",
    "plt.plot(df_stats['Valid. Accur.'], 'r-o', label=\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.xticks([i for i in range(len(df_stats))], df_stats['epoch'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28906e1",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL Má»šI: Váº¼ CONFUSION MATRIX ---\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# TÃ­nh toÃ¡n ma tráº­n nháº§m láº«n dá»±a trÃªn Epoch cuá»‘i cÃ¹ng\n",
    "cm = confusion_matrix(val_true, val_preds)\n",
    "\n",
    "# Váº½ Heatmap\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Fake', 'Real'], \n",
    "            yticklabels=['Fake', 'Real'])\n",
    "\n",
    "plt.ylabel('Thá»±c táº¿ (Ground Truth)')\n",
    "plt.xlabel('Dá»± Ä‘oÃ¡n (Prediction)')\n",
    "plt.title('Confusion Matrix - Káº¿t quáº£ phÃ¢n loáº¡i')\n",
    "plt.show()\n",
    "\n",
    "# In láº¡i bÃ¡o cÃ¡o chi tiáº¿t\n",
    "print(classification_report(val_true, val_preds, target_names=['Fake', 'Real']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ddb20",
   "metadata": {},
   "source": [
    "# LÆ°u Model & Test thá»­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab892b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_news_logic(text, threshold=0.80): # NgÆ°á»¡ng tin cáº­y 80%\n",
    "    # 1. Chuáº©n bá»‹ dá»¯ liá»‡u\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128, padding='max_length')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # 2. Model dá»± Ä‘oÃ¡n\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        \n",
    "    fake_prob = probs[0][0].item() # XÃ¡c suáº¥t lÃ  Fake (Label 0)\n",
    "    real_prob = probs[0][1].item() # XÃ¡c suáº¥t lÃ  Real (Label 1)\n",
    "    \n",
    "    print(f\"\\nğŸ“° Input: {text[:50]}...\")\n",
    "    print(f\"ğŸ“Š Chá»‰ sá»‘: Real={real_prob:.2f} | Fake={fake_prob:.2f}\")\n",
    "\n",
    "    # 3. LOGIC QUYáº¾T Äá»ŠNH (Quan trá»ng nháº¥t)\n",
    "    # Náº¿u cháº¯c cháº¯n lÃ  Real (> 80%)\n",
    "    if real_prob > threshold:\n",
    "        return 1  # Label REAL\n",
    "        \n",
    "    # Náº¿u cháº¯c cháº¯n lÃ  Fake (> 80%)\n",
    "    elif fake_prob > threshold:\n",
    "        return 0  # Label FAKE\n",
    "        \n",
    "    # Náº¿u láº¥p lá»­ng (khÃ´ng bÃªn nÃ o > 80%)\n",
    "    else:\n",
    "        return 2  # Label UNDEFINED (Há»‡ thá»‘ng tá»± gÃ¡n)\n",
    "\n",
    "# --- TEST THá»¬ ---\n",
    "# Case 1: Ráº¥t rÃµ rÃ ng\n",
    "kq1 = predict_news_logic(\"Bá»™ Y táº¿ chÃ­nh thá»©c phÃª duyá»‡t váº¯c xin má»›i.\") \n",
    "print(f\"ğŸ‘‰ Káº¿t quáº£ há»‡ thá»‘ng tráº£ vá»: {kq1}\") \n",
    "\n",
    "# Case 2: MÆ¡ há»“ / Bá»‹a Ä‘áº·t láº¡ láº«m\n",
    "kq2 = predict_news_logic(\"NgÆ°á»i ngoÃ i hÃ nh tinh Ä‘ang Äƒn bÃºn Ä‘áº­u máº¯m tÃ´m.\")\n",
    "print(f\"ğŸ‘‰ Káº¿t quáº£ há»‡ thá»‘ng tráº£ vá»: {kq2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news-detection (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
