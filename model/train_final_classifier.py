import pandas as pd
import numpy as np
import joblib
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import random

# --- 1. Táº O Dá»® LIá»†U FEATURE GIáº¢ Láº¬P (MÃ” PHá»NG PIPELINE) ---
# VÃ¬ cháº¡y pipeline tháº­t trÃªn hÃ ng nghÃ¬n bÃ i sáº½ ráº¥t lÃ¢u, 
# ta mÃ´ phá»ng láº¡i cÃ¡c chá»‰ sá»‘ mÃ  Pipeline cá»§a báº¡n vá»«a xuáº¥t ra á»Ÿ trÃªn.

def generate_mock_features(n_samples=1000):
    data = []
    
    # A. Sinh dá»¯ liá»‡u cho bÃ i REAL (Label = 1)
    for _ in range(n_samples // 2):
        # BÃ i tháº­t thÆ°á»ng cÃ³:
        # - CÃ³ cÃ¢u Ä‘Æ°á»£c Support (Score > 0.7)
        # - Ráº¥t Ã­t cÃ¢u bá»‹ Refute (Score < 0.3)
        # - Avg score cao
        
        n_claims = random.randint(3, 10)
        scores = []
        for _ in range(n_claims):
            # 80% lÃ  support, 20% lÃ  neutral/noise
            if random.random() < 0.8:
                scores.append(random.uniform(0.75, 0.99)) # High score
            else:
                scores.append(random.uniform(0.4, 0.6))   # Neutral
        
        # TÃ­nh feature
        avg_score = np.mean(scores)
        min_score = np.min(scores) # BÃ i tháº­t thÃ¬ min score váº«n thÆ°á»ng > 0.4
        supported_ratio = sum(1 for s in scores if s > 0.7) / n_claims
        refuted_ratio = sum(1 for s in scores if s < 0.25) / n_claims # ThÆ°á»ng lÃ  0
        
        data.append([avg_score, min_score, supported_ratio, refuted_ratio, 1])

    # B. Sinh dá»¯ liá»‡u cho bÃ i FAKE (Label = 0)
    for _ in range(n_samples // 2):
        # BÃ i giáº£ thÆ°á»ng cÃ³:
        # - Ãt nháº¥t 1 cÃ¢u bá»‹ Refute (Score cá»±c tháº¥p ~0.002)
        # - Avg score tháº¥p
        
        n_claims = random.randint(3, 10)
        scores = []
        # Cháº¯c cháº¯n cÃ³ 1-2 cÃ¢u nÃ³i Ä‘iÃªu
        scores.append(random.uniform(0.001, 0.1)) 
        if random.random() < 0.5: scores.append(random.uniform(0.001, 0.1))
        
        # CÃ²n láº¡i cÃ³ thá»ƒ lÃ  cÃ¢u dáº«n (neutral) hoáº·c cÃ¢u Ä‘Ãºng 1 ná»­a
        for _ in range(n_claims - len(scores)):
            scores.append(random.uniform(0.3, 0.8))
            
        # TÃ­nh feature
        avg_score = np.mean(scores)
        min_score = np.min(scores) # BÃ i giáº£ thÃ¬ min score cá»±c tháº¥p
        supported_ratio = sum(1 for s in scores if s > 0.7) / n_claims
        refuted_ratio = sum(1 for s in scores if s < 0.25) / n_claims # > 0
        
        data.append([avg_score, min_score, supported_ratio, refuted_ratio, 0])

    df = pd.DataFrame(data, columns=['avg_score', 'min_score', 'supported_ratio', 'refuted_ratio', 'label'])
    return df

# --- 2. TRAIN XGBOOST (Gradient Boosting) ---
def train_classifier():
    print("ğŸ› ï¸ Äang sinh dá»¯ liá»‡u mÃ´ phá»ng Pipeline...")
    df = generate_mock_features(2000) # 2000 máº«u
    
    X = df.drop(columns=['label'])
    y = df['label']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    print("ğŸš€ Äang train Gradient Boosting Classifier...")
    # XGBoost lÃ  thuáº­t toÃ¡n cá»±c máº¡nh cho dáº¡ng dá»¯ liá»‡u báº£ng nÃ y
    clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)
    clf.fit(X_train, y_train)
    
    # ÄÃ¡nh giÃ¡
    y_pred = clf.predict(X_test)
    print("\n" + "="*30)
    print(f"âœ… Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print("="*30)
    print(classification_report(y_test, y_pred))
    
    # LÆ°u model
    joblib.dump(clf, 'final_classifier.pkl')
    print("ğŸ’¾ ÄÃ£ lÆ°u model táº¡i 'final_classifier.pkl'")

if __name__ == "__main__":
    train_classifier()