import json
import os
import sys
import time
import torch
import psycopg2
import numpy as np
from kafka import KafkaConsumer
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from underthesea import sent_tokenize
from dotenv import load_dotenv

load_dotenv()

# --- Cáº¤U HÃŒNH ---
KAFKA_TOPIC = "raw_articles"
KAFKA_SERVER = os.getenv("KAFKA_SERVER", "localhost:9092")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 32 # Tá»‘i Æ°u tá»‘c Ä‘á»™ xá»­ lÃ½ hÃ ng loáº¡t

# Cáº¥u hÃ¬nh DB
DB_CONFIG = {
    "dbname": os.getenv("POSTGRES_DB", "vnexpress_scraper"),
    "user": os.getenv("POSTGRES_USER", "admin"),
    "password": os.getenv("POSTGRES_PASSWORD", "admin"),
    "host": os.getenv("POSTGRES_HOST", "localhost"),
    "port": os.getenv("POSTGRES_PORT", "5432")
}

MODEL_EXTRACTOR_PATH = "model/phobert_claim_extractor"

class AIProcessor:
    def __init__(self):
        print(f"ğŸš€ [Consumer] KHá»I Äá»˜NG AI PROCESSOR TRÃŠN {DEVICE.upper()}...")
        
        # 1. Load Model Lá»c CÃ¢u (Claim Extractor)
        print("   â”œâ”€ [1/3] Loading Claim Extractor (PhoBERT)...")
        try:
            self.ext_tokenizer = AutoTokenizer.from_pretrained(MODEL_EXTRACTOR_PATH)
            self.ext_model = AutoModelForSequenceClassification.from_pretrained(MODEL_EXTRACTOR_PATH).to(DEVICE)
            self.ext_model.eval()
        except Exception as e:
            # RAISE ERROR Ä‘á»ƒ Docker biáº¿t mÃ  restart, khÃ´ng exit() Ã¢m tháº§m
            raise RuntimeError(f"âŒ Lá»—i load model extractor: {e}. HÃ£y kiá»ƒm tra path '{MODEL_EXTRACTOR_PATH}'")

        # 2. Load Model Embedding (Bi-Encoder)
        print("   â”œâ”€ [2/3] Loading Embedding Model (Bi-Encoder)...")
        try:
            self.embedder = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder', device=DEVICE)
        except Exception as e:
            raise RuntimeError(f"âŒ Lá»—i load model embedding: {e}")

        # 3. Káº¿t ná»‘i DB
        print("   â”œâ”€ [3/3] Connecting to PostgreSQL...")
        self.connect_db()
        print("âœ… Há»† THá»NG Sáº´N SÃ€NG Xá»¬ LÃ!")

    def connect_db(self):
        """HÃ m káº¿t ná»‘i DB cÃ³ kháº£ nÄƒng reconnect"""
        try:
            self.conn = psycopg2.connect(**DB_CONFIG)
            self.conn.autocommit = True # Tá»± Ä‘á»™ng commit Ä‘á»ƒ trÃ¡nh lock lÃ¢u
        except Exception as e:
            raise ConnectionError(f"âŒ KhÃ´ng thá»ƒ káº¿t ná»‘i DB: {e}")

    def extract_claims(self, text):
        """TÃ¡ch cÃ¢u vÃ  dÃ¹ng AI lá»c ra nhá»¯ng cÃ¢u Ä‘Ã¡ng check"""
        if not text: return []
        
        # BÆ°á»›c 1: TÃ¡ch cÃ¢u (Heuristic)
        sentences = sent_tokenize(text)
        # Filter sÆ¡ bá»™: CÃ¢u > 5 tá»«
        candidates = [s.strip() for s in sentences if len(s.split()) > 5]
        
        if not candidates: return []

        # BÆ°á»›c 2: Cháº¡y qua Model Extractor (AI Classifier)
        # Tokenize batch
        inputs = self.ext_tokenizer(
            candidates, return_tensors="pt", padding=True, truncation=True, max_length=128
        ).to(DEVICE)
        
        with torch.no_grad():
            outputs = self.ext_model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            preds = torch.argmax(probs, dim=1).cpu().numpy()
        
        # Chá»‰ láº¥y cÃ¢u cÃ³ nhÃ£n 1 (CLAIM)
        return [candidates[i] for i, pred in enumerate(preds) if pred == 1]

    def process_message(self, article):
        """Xá»­ lÃ½ 1 bÃ i bÃ¡o (Pipeline: Article -> Claims -> Embeddings -> DB)"""
        url = article.get('url')
        title = article.get('title')
        content = article.get('content')
        published_at = article.get('published_date')
        category = article.get('category', 'General')

        # Sá»­ dá»¥ng Context Manager cho Cursor (Best Practice)
        with self.conn.cursor() as cur:
            # 1. LÆ°u Article (Ingestion)
            try:
                # Kiá»ƒm tra xem bÃ i Ä‘Ã£ tá»“n táº¡i chÆ°a
                cur.execute("SELECT id FROM articles WHERE url = %s", (url,))
                existing = cur.fetchone()
                
                if existing:
                    article_id = existing[0]
                    # Update ná»™i dung náº¿u crawl láº¡i
                    cur.execute("""
                        UPDATE articles 
                        SET content = %s, scraped_at = NOW(), published_date = %s, category = %s
                        WHERE id = %s
                    """, (content, published_at, category, article_id))
                else:
                    # Insert bÃ i má»›i
                    cur.execute("""
                        INSERT INTO articles (url, title, content, published_date, category, scraped_at)
                        VALUES (%s, %s, %s, %s, %s, NOW())
                        RETURNING id;
                    """, (url, title, content, published_at, category))
                    article_id = cur.fetchone()[0]

            except Exception as e:
                print(f"   âŒ Lá»—i DB Article: {e}")
                return # Bá» qua bÃ i nÃ y náº¿u lá»—i DB

            # 2. Feature Extraction (Claim Extraction)
            claims = self.extract_claims(content)
            if not claims:
                print(f"   â„¹ï¸ KhÃ´ng tÃ¬m tháº¥y claim: {title[:40]}...")
                return

            # 3. Vectorization (Embedding) - Batch Processing
            print(f"   ğŸ” Vector hÃ³a {len(claims)} claims...")
            embeddings = self.embedder.encode(
                claims, 
                batch_size=BATCH_SIZE, 
                show_progress_bar=False,
                convert_to_numpy=True
            )
            
            # 4. Storage (LÆ°u Claims vá»›i nhÃ£n UNDEFINED)
            count = 0
            for text, emb in zip(claims, embeddings):
                try:
                    # LÆ°u vector dáº¡ng list (pgvector tá»± hiá»ƒu)
                    cur.execute("""
                        INSERT INTO claims (article_id, content, embedding, system_label, verified, source_type)
                        VALUES (%s, %s, %s, 'REAL', TRUE, 'article') 
                    """, (article_id, text, emb.tolist()))
                    count += 1
                except Exception as e:
                    print(f"      âŒ Lá»—i lÆ°u claim con: {e}")

            if count > 0:
                print(f"   âœ… [Processed] {title[:40]}... -> {count} Claims lÆ°u DB.")

    def start_consuming(self):
        print(f"\nğŸ“¡ [Consumer] ÄANG Láº®NG NGHE TOPIC '{KAFKA_TOPIC}'...")
        
        while True:
            try:
                consumer = KafkaConsumer(
                    KAFKA_TOPIC,
                    bootstrap_servers=[KAFKA_SERVER],
                    auto_offset_reset='earliest', # Äá»c tá»« Ä‘áº§u náº¿u lÃ  group má»›i
                    enable_auto_commit=True,
                    group_id='ai-processor-group-v2', # âœ¨ Äá»•i version Ä‘á»ƒ Ä‘á»c láº¡i tá»« Ä‘áº§u
                    value_deserializer=lambda x: json.loads(x.decode('utf-8')),
                    consumer_timeout_ms=10000,  # Timeout 10s Ä‘á»ƒ cÃ³ thá»ƒ show heartbeat
                    # Tá»‘i Æ°u fetch
                    fetch_min_bytes=1,  # Giáº£m xuá»‘ng Ä‘á»ƒ nháº­n ngay khi cÃ³ message
                    fetch_max_wait_ms=500
                )
                
                print("âœ… [Consumer] Kafka Connected!")
                print(f"   Consumer Group: ai-processor-group-v2")
                print(f"   Auto Offset Reset: earliest\n")
                
                message_count = 0
                heartbeat_count = 0
                
                while True:
                    try:
                        for message in consumer:
                            message_count += 1
                            print(f"ğŸ“¥ [{message_count}] Nháº­n bÃ i tá»« offset {message.offset}: {message.value.get('title', 'N/A')[:50]}...")
                            self.process_message(message.value)
                    except StopIteration:
                        # Timeout - no messages received
                        heartbeat_count += 1
                        print(f"ğŸ’“ [Heartbeat #{heartbeat_count}] Äang chá» tin nháº¯n má»›i... (ÄÃ£ xá»­ lÃ½: {message_count} bÃ i)")
                        continue

            except Exception as e:
                print(f"âŒ [Consumer] Lá»—i káº¿t ná»‘i Kafka: {e}")
                print("â³ Thá»­ láº¡i sau 5s...")
                time.sleep(5)

if __name__ == "__main__":
    # Äáº£m báº£o DB sáºµn sÃ ng trÆ°á»›c khi cháº¡y
    # (Trong Production sáº½ dÃ¹ng healthcheck container)
    
    # NOTE: Náº¿u consumer Ä‘Ã£ cháº¡y trÆ°á»›c Ä‘Ã³ vÃ  Ä‘Ã£ Ä‘á»c háº¿t messages, 
    # nÃ³ sáº½ tiáº¿p tá»¥c tá»« offset cÅ©. Äá»ƒ Ä‘á»c láº¡i tá»« Ä‘áº§u:
    # 1. Äá»•i group_id trong code (VD: 'ai-processor-group-v2')
    # 2. Hoáº·c reset offset: kafka-consumer-groups --bootstrap-server localhost:9092 --group ai-processor-group-v1 --reset-offsets --to-earliest --execute --topic raw_articles
    
    processor = AIProcessor()
    processor.start_consuming()