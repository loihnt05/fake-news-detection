import psycopg2
import torch
import os
import sys
from tqdm import tqdm
from underthesea import sent_tokenize
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

load_dotenv()

# --- C·∫§U H√åNH ---
DB_CONFIG = {
    "dbname": os.getenv("POSTGRES_DB", "vnexpress_scraper"),
    "user": os.getenv("POSTGRES_USER", "admin"),
    "password": os.getenv("POSTGRES_PASSWORD", "admin"),
    "host": os.getenv("POSTGRES_HOST", "localhost"),
    "port": os.getenv("POSTGRES_PORT", "5432")
}

# ƒê∆Ø·ªúNG D·∫™N MODEL M·ªöI (Theo c·∫•u tr√∫c project hi·ªán t·∫°i)
MODEL_CLAIM_PATH = "model/phobert_claim_extractor" 
MODEL_EMBED_PATH = "bkai-foundation-models/vietnamese-bi-encoder"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 32  # X·ª≠ l√Ω 32 c√¢u m·ªôt l√∫c cho nhanh

class KnowledgeBaseRebuilder:
    def __init__(self):
        print(f"üöÄ [Rebuilder] KNOWLEDGE BASE ƒêANG ƒê∆Ø·ª¢C X√ÇY D·ª∞NG L·∫†I TR√äN {DEVICE.upper()}...")

        # 1. Load Model L·ªçc Claim (Ng∆∞·ªùi g√°c c·ªïng)
        print("   ‚îú‚îÄ [1/2] Loading Claim Detector...")
        try:
            self.claim_tokenizer = AutoTokenizer.from_pretrained(MODEL_CLAIM_PATH)
            self.claim_model = AutoModelForSequenceClassification.from_pretrained(MODEL_CLAIM_PATH)
            self.claim_model.to(DEVICE)
            self.claim_model.eval()
        except Exception as e:
            print(f"‚ùå L·ªói load model Claim: {e}")
            print(f"üëâ H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ ƒë·ªÉ model t·∫°i: {MODEL_CLAIM_PATH}")
            sys.exit(1)

        # 2. Load Model Vector (Ng∆∞·ªùi m√£ h√≥a)
        print("   ‚îú‚îÄ [2/2] Loading Embedding Model...")
        self.embed_model = SentenceTransformer(MODEL_EMBED_PATH, device=DEVICE)
        
        # 3. K·∫øt n·ªëi DB
        self.conn = psycopg2.connect(**DB_CONFIG)
        self.conn.autocommit = True

    def get_raw_articles(self):
        """L·∫•y t·∫•t c·∫£ b√†i b√°o t·ª´ b·∫£ng articles"""
        with self.conn.cursor() as cur:
            # L·∫•y ID v√† Content c·ªßa b√†i b√°o
            # (Kh√¥ng c·∫ßn WHERE label=1 n·ªØa v√¨ ta m·∫∑c ƒë·ªãnh ngu·ªìn c√†o v·ªÅ l√† tin c·∫≠y)
            cur.execute("SELECT id, content FROM articles WHERE label='1' and content IS NOT NULL")
            return cur.fetchall()

    def predict_batch(self, texts):
        """D·ª± ƒëo√°n nhanh m·ªôt l√¥ c√¢u h·ªèi (Batch Inference)"""
        inputs = self.claim_tokenizer(
            texts, padding=True, truncation=True, max_length=128, return_tensors="pt"
        ).to(DEVICE)
        
        with torch.no_grad():
            outputs = self.claim_model(**inputs)
            # L·∫•y nh√£n c√≥ x√°c su·∫•t cao nh·∫•t (0 ho·∫∑c 1)
            preds = torch.argmax(outputs.logits, dim=1)
        return preds.cpu().numpy()

    def run(self):
        # 1. D·ªåN D·∫∏P D·ªÆ LI·ªÜU C≈®
        print("\nüßπ ƒêang d·ªçn d·∫πp b·∫£ng 'claims' c≈©...")
        with self.conn.cursor() as cur:
            # X√≥a user_reports tr∆∞·ªõc v√¨ n√≥ tham chi·∫øu ƒë·∫øn claims
            cur.execute("TRUNCATE TABLE user_reports CASCADE;")
            cur.execute("TRUNCATE TABLE claims CASCADE;")
        print("‚úÖ Database ƒë√£ s·∫°ch.")

        # 2. L·∫§Y D·ªÆ LI·ªÜU NGU·ªíN
        articles = self.get_raw_articles()
        print(f"üì¶ T√¨m th·∫•y {len(articles)} b√†i b√°o g·ªëc. B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t...")

        total_claims_saved = 0
        
        # Bi·∫øn t·∫°m ƒë·ªÉ gom batch vector h√≥a
        pending_insert = [] # List c√°c tuple (article_id, content)

        # 3. V√íNG L·∫∂P X·ª¨ L√ù
        for art_id, content in tqdm(articles, desc="Processing"):
            # A. T√°ch c√¢u
            sentences = sent_tokenize(content)
            # L·ªçc s∆° b·ªô c√¢u qu√° ng·∫Øn (< 5 t·ª´)
            candidates = [s.strip() for s in sentences if len(s.split()) > 5]
            
            if not candidates: continue

            # B. AI L·ªçc (Batch Processing)
            # Chia nh·ªè candidates th√†nh c√°c batch nh·ªè h∆°n n·∫øu qu√° nhi·ªÅu c√¢u
            for i in range(0, len(candidates), BATCH_SIZE):
                batch_text = candidates[i : i + BATCH_SIZE]
                
                # Model ph√°n x√©t: 1=Claim, 0=Non-Claim
                labels = self.predict_batch(batch_text)
                
                # Ch·ªâ l·∫•y c√¢u Label 1
                for text, label in zip(batch_text, labels):
                    if label == 1:
                        pending_insert.append((art_id, text))

            # C. Vector h√≥a & L∆∞u (Khi gom ƒë·ªß l∆∞·ª£ng l·ªõn ho·∫∑c h·∫øt b√†i)
            # Gom kho·∫£ng 64 c√¢u r·ªìi x·ª≠ l√Ω 1 l·∫ßn cho t·ªëi ∆∞u GPU
            if len(pending_insert) >= 64:
                self.flush_to_db(pending_insert)
                total_claims_saved += len(pending_insert)
                pending_insert = [] # Reset

        # X·ª≠ l√Ω n·ªët ph·∫ßn c√≤n d∆∞
        if pending_insert:
            self.flush_to_db(pending_insert)
            total_claims_saved += len(pending_insert)

        print(f"\nüéâ HO√ÄN T·∫§T! ƒê√£ x√¢y d·ª±ng Knowledge Base v·ªõi {total_claims_saved} claims ch·∫•t l∆∞·ª£ng.")
        self.conn.close()

    def flush_to_db(self, items):
        """Vector h√≥a v√† Insert v√†o DB"""
        if not items: return
        
        # T√°ch list tuple th√†nh 2 list ri√™ng
        art_ids = [x[0] for x in items]
        texts = [x[1] for x in items]
        
        # Vector h√≥a h√†ng lo·∫°t
        embeddings = self.embed_model.encode(texts, batch_size=BATCH_SIZE, show_progress_bar=False)
        
        # Insert Bulk
        with self.conn.cursor() as cur:
            # Chu·∫©n b·ªã d·ªØ li·ªáu cho execute_values ho·∫∑c loop
            # ·ªû ƒë√¢y d√πng loop ƒë∆°n gi·∫£n v√¨ psycopg2 x·ª≠ l√Ω kh√° nhanh
            insert_args = []
            for mid, txt, emb in zip(art_ids, texts, embeddings):
                # QUAN TR·ªåNG: G√°n nh√£n REAL
                insert_args.append((mid, txt, emb.tolist(), 'REAL', True))
            
            # S·ª≠ d·ª•ng executemany ƒë·ªÉ insert nhanh
            query = """
                INSERT INTO claims (article_id, content, embedding, system_label, verified, source_type)
                VALUES (%s, %s, %s, %s, %s, 'article')
            """
            cur.executemany(query, insert_args)

if __name__ == "__main__":
    rebuilder = KnowledgeBaseRebuilder()
    rebuilder.run()