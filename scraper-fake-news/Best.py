import pandas as pd
import random
import re
import time
import numpy as np
import nltk
import sqlite3
import uuid
from pyvi import ViTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Tuple
from urllib.parse import urlparse

# --- PART 1: SETUP & NLP UTILITIES ---
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    nltk.download('punkt_tab')

VIETNAMESE_STOPWORDS = {
    "lÃ ", "vÃ ", "cá»§a", "thÃ¬", "mÃ ", "á»Ÿ", "bá»‹", "Ä‘Æ°á»£c", "cho", "vá»", "vá»›i",
    "nhá»¯ng", "cÃ¡c", "cÃ³", "lÃ m", "láº¡i", "ngÆ°á»i", "nÃ y", "Ä‘Ã³", "ra", "Ä‘Ã£", 
    "Ä‘ang", "sáº½", "pháº£i", "nhÆ°", "nhÆ°ng", "tá»«", "vÃ¬", "theo", "khi", "Ä‘á»ƒ", 
    "trÃªn", "dÆ°á»›i", "trong", "ngoÃ i", "táº¡i", "hay", "hoáº·c", "cÅ©ng", "ráº¥t", 
    "nhiá»u", "toÃ n", "bá»™", "nháº¥t", "hÆ¡n", "chá»‰", "váº«n", "cÃ¹ng", "viá»‡c"
}

# Tá»ª ÄIá»‚N Äá»’NG NGHÄ¨A (Sá»­ dá»¥ng tá»« ghÃ©p cÃ³ dáº¥u gáº¡ch dÆ°á»›i _ Ä‘á»ƒ ViTokenizer xá»­ lÃ½ Ä‘Ãºng)
# ÄÃ£ xÃ³a cÃ¡c tá»« Ä‘Æ¡n nguy hiá»ƒm nhÆ° "tÄƒng", "giáº£m" Ä‘á»ƒ trÃ¡nh lá»—i "gia tÄƒng cÆ°á»ng"
VIETNAMESE_SYNONYMS = {
    "sá»­_dá»¥ng": ["dÃ¹ng", "Ã¡p_dá»¥ng", "váº­n_dá»¥ng"],
    "phÃ¡t_triá»ƒn": ["má»Ÿ_rá»™ng", "tÄƒng_trÆ°á»Ÿng", "vÆ°Æ¡n_lÃªn"],
    "quan_trá»ng": ["thiáº¿t_yáº¿u", "cá»‘t_lÃµi", "trá»ng_yáº¿u", "then_chá»‘t"],
    "thÃ´ng_bÃ¡o": ["cÃ´ng_bá»‘", "tuyÃªn_bá»‘", "cho_hay", "Ä‘Æ°a_tin"],
    "xáº£y_ra": ["diá»…n_ra", "xuáº¥t_hiá»‡n", "bÃ¹ng_phÃ¡t"],
    "váº¥n_Ä‘á»": ["thá»±c_tráº¡ng", "tÃ¬nh_hÃ¬nh", "sá»±_viá»‡c", "váº¥n_náº¡n"],
    "há»—_trá»£": ["giÃºp_Ä‘á»¡", "trá»£_giÃºp", "tiáº¿p_sá»©c"],
    "ngÆ°á»i_dÃ¢n": ["bÃ _con", "cÃ´ng_chÃºng", "quáº§n_chÃºng", "nhÃ¢n_dÃ¢n"],
    "chÃ­nh_phá»§": ["nhÃ _nÆ°á»›c", "chÃ­nh_quyá»n", "cÆ¡_quan_chá»©c_nÄƒng"],
    "tÄƒng_cÆ°á»ng": ["Ä‘áº©y_máº¡nh", "gia_tÄƒng", "cá»§ng_cá»‘", "tháº¯t_cháº·t"], # Sá»­a lá»—i "gia tÄƒng cÆ°á»ng"
    "cáº£i_thiá»‡n": ["nÃ¢ng_cao", "hoÃ n_thiá»‡n", "tá»‘t_hÆ¡n"],
    "yÃªu_cáº§u": ["Ä‘á»_nghá»‹", "Ä‘Ã²i_há»i", "mong_muá»‘n"],
    "thá»±c_hiá»‡n": ["triá»ƒn_khai", "tiáº¿n_hÃ nh", "thi_hÃ nh"],
    "liÃªn_tá»¥c": ["thÆ°á»ng_xuyÃªn", "liÃªn_tiáº¿p", "dá»“n_dáº­p"]
}

# --- FAKE URL GENERATOR ---
class FakeURLGenerator:
    def __init__(self):
        self.keyboard_map = {
            'q': 'wa', 'w': 'qase', 'e': 'wsrd', 'r': 'edft', 't': 'rfgy', 'y': 'tghu', 'u': 'yhij', 'i': 'ujko', 'o': 'iklp', 'p': 'ol',
            'a': 'qwsz', 's': 'awedxz', 'd': 'serfcx', 'f': 'drtgv', 'g': 'ftyhb', 'h': 'gyunj', 'j': 'hukm', 'k': 'jilo', 'l': 'kop',
            'z': 'asx', 'x': 'zsdc', 'c': 'xdfv', 'v': 'cfgb', 'b': 'vghn', 'n': 'bhjm', 'm': 'njk'
        }
        self.visual_map = {
            'v': ['u'], 'n': ['m', 'h'], 'e': ['c', 'o'], 'x': ['c', 'k', 'z'], 'p': ['q', 'o'], 'r': ['n'], 's': ['z', '5'], 'o': ['0', 'c'], 'a': ['e'], 'd': ['cl']
        }
        self.domains = [".com.vn", ".net", ".gov.vn", ".vn", ".com", ".org"]
    
    def _apply_typo(self, base_word: str) -> str:
        techniques = [self._substitution, self._omission, self._duplication, self._transposition, self._visual_spoof]
        technique = random.choice(techniques)
        result = technique(base_word)
        return result if result else base_word
    
    def _substitution(self, word: str) -> str:
        if len(word) == 0: return word
        idx = random.randint(0, len(word) - 1)
        char = word[idx]
        if char in self.keyboard_map:
            replacement = random.choice(self.keyboard_map[char])
            return word[:idx] + replacement + word[idx+1:]
        return word
    
    def _omission(self, word: str) -> str:
        if len(word) <= 3: return word
        idx = random.randint(0, len(word) - 1)
        return word[:idx] + word[idx+1:]
    
    def _duplication(self, word: str) -> str:
        if len(word) == 0: return word
        idx = random.randint(0, len(word) - 1)
        return word[:idx] + word[idx] + word[idx] + word[idx+1:]
    
    def _transposition(self, word: str) -> str:
        if len(word) <= 1: return word
        idx = random.randint(0, len(word) - 2)
        chars = list(word)
        chars[idx], chars[idx+1] = chars[idx+1], chars[idx]
        return "".join(chars)
    
    def _visual_spoof(self, word: str) -> str:
        if len(word) == 0: return word
        idx = random.randint(0, len(word) - 1)
        char = word[idx]
        if char in self.visual_map:
            replacement = random.choice(self.visual_map[char])
            return word[:idx] + replacement + word[idx+1:]
        return word
    
    def generate_fake_url(self, original_url: str) -> str:
        if not original_url or original_url == '': return ''
        try:
            parsed = urlparse(original_url)
            domain_parts = parsed.netloc.split('.')
            if len(domain_parts) == 0: return original_url
            base_domain = domain_parts[0]
            fake_domain = self._apply_typo(base_domain)
            fake_extension = random.choice(self.domains)
            fake_url = f"{parsed.scheme}://{fake_domain}{fake_extension}"
            return fake_url
        except Exception:
            return original_url

# --- PART 2: SALIENCY AND DISINFORMATION GENERATION ---

def get_most_impactful_sentence(text: str) -> str:
    sentences = nltk.sent_tokenize(text)
    valid_sentences = [s for s in sentences if len(s.split()) > 5]
    
    if not valid_sentences:
        return "" if not sentences else sentences[0]
    
    if len(valid_sentences) == 1:
        return valid_sentences[0]

    processed_sentences = [ViTokenizer.tokenize(s) for s in valid_sentences]
    vectorizer = TfidfVectorizer(stop_words=list(VIETNAMESE_STOPWORDS), token_pattern=r'(?u)\b\w+\b')
    try:
        tfidf_matrix = vectorizer.fit_transform(processed_sentences)
    except ValueError:
        return max(valid_sentences, key=len)

    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)
    sentence_scores = np.sum(similarity_matrix, axis=1)
    top_idx = np.argmax(sentence_scores)
    
    return valid_sentences[top_idx]

def alter_numbers(text: str) -> str:
    """
    TÃ¬m vÃ  thay Ä‘á»•i cÃ¡c con sá»‘ NHÆ¯NG bá» qua ngÃ y thÃ¡ng, giá» giáº¥c.
    """
    def replace_num(match):
        original = match.group()
        try:
            num = int(original)
            # Bá» qua cÃ¡c sá»‘ trÃ´ng giá»‘ng nÄƒm (19xx, 20xx)
            if 1900 <= num <= 2100:
                return original
            
            # Thay Ä‘á»•i giÃ¡ trá»‹ ngáº«u nhiÃªn
            change = random.choice([0.5, 0.8, 1.2, 1.5, 2.0])
            new_num = int(num * change)
            return str(new_num)
        except ValueError:
            return original

    # Regex cáº£i tiáº¿n:
    # (?<![\d\/\-\.]) : KhÃ´ng Ä‘Æ°á»£c cÃ³ sá»‘, dáº¥u /, -, . Ä‘á»©ng trÆ°á»›c
    # \b\d{2,3}\b     : TÃ¬m sá»‘ cÃ³ 2-3 chá»¯ sá»‘
    # (?![\d\/\-\.])  : KhÃ´ng Ä‘Æ°á»£c cÃ³ sá»‘, dáº¥u /, -, . Ä‘á»©ng sau
    # Äiá»u nÃ y sáº½ giÃºp trÃ¡nh 24/4, 20-10, 15.5
    pattern = r'(?<![\d\/\-\.])\b\d{2,3}\b(?![\d\/\-\.])'
    return re.sub(pattern, replace_num, text)

def paraphrase_with_synonyms(text: str) -> str:
    """
    Sá»­ dá»¥ng ViTokenizer Ä‘á»ƒ giá»¯ nguyÃªn tá»« ghÃ©p trÆ°á»›c khi thay tháº¿.
    """
    # 1. Tokenize (vÃ­ dá»¥: "tÄƒng cÆ°á»ng kháº£ nÄƒng" -> "tÄƒng_cÆ°á»ng kháº£_nÄƒng")
    tokenized_text = ViTokenizer.tokenize(text)
    tokens = tokenized_text.split()
    
    new_tokens = []
    for token in tokens:
        # Kiá»ƒm tra token (cÃ³ gáº¡ch dÆ°á»›i) vá»›i tá»« Ä‘iá»ƒn
        lower_token = token.lower()
        if lower_token in VIETNAMESE_SYNONYMS and random.random() > 0.6:
            replacement = random.choice(VIETNAMESE_SYNONYMS[lower_token])
            # Giá»¯ Ä‘á»‹nh dáº¡ng token Ä‘á»ƒ ná»‘i láº¡i sau nÃ y (náº¿u thay tháº¿ cÅ©ng lÃ  tá»« ghÃ©p)
            new_tokens.append(replacement)
        else:
            new_tokens.append(token)
            
    # Ná»‘i láº¡i vÃ  thay tháº¿ gáº¡ch dÆ°á»›i báº±ng khoáº£ng tráº¯ng
    return " ".join(new_tokens).replace('_', ' ')

def flip_sentence_meaning(sentence: str) -> str:
    antonyms = {
        "tÄƒng": "giáº£m", "tÄƒng trÆ°á»Ÿng": "suy thoÃ¡i", "phÃ¡t triá»ƒn": "Ä‘Ã¬nh trá»‡",
        "nÃ¢ng cao": "háº¡ tháº¥p", "cáº£i thiá»‡n": "lÃ m tráº§m trá»ng", "má»Ÿ rá»™ng": "thu háº¹p",
        "thÃ nh cÃ´ng": "tháº¥t báº¡i", "hiá»‡u quáº£": "vÃ´ tÃ¡c dá»¥ng", 
        "á»§ng há»™": "pháº£n Ä‘á»‘i ká»‹ch liá»‡t", "Ä‘á»“ng Ã½": "bÃ¡c bá»", "cháº¥p thuáº­n": "tá»« chá»‘i",
        "tÃ­ch cá»±c": "tiÃªu cá»±c", "láº¡c quan": "bi quan", "kháº£ quan": "Ä‘Ã¡ng bÃ¡o Ä‘á»™ng",
        "tá»‘t": "tá»“i tá»‡", "cao": "tháº¥p ká»· lá»¥c", "máº¡nh": "yáº¿u kÃ©m",
        "an toÃ n": "cá»±c ká»³ nguy hiá»ƒm", "á»•n Ä‘á»‹nh": "báº¥t á»•n Ä‘á»‹nh", "tin cáº­y": "gian dá»‘i",
        "rÃµ rÃ ng": "máº­p má»", "chÃ­nh xÃ¡c": "sai lá»‡ch hoÃ n toÃ n", "Ä‘Ãºng": "sai",
        "nhiá»u": "ráº¥t Ã­t", "Ä‘a sá»‘": "thiá»ƒu sá»‘", "táº¥t cáº£": "khÃ´ng ai",
        "luÃ´n": "khÃ´ng bao giá»", "thÆ°á»ng xuyÃªn": "hiáº¿m khi",
        "kháº³ng Ä‘á»‹nh": "phá»§ nháº­n", "xÃ¡c nháº­n": "bÃ¡c bá» thÃ´ng tin",
        "hoÃ n thÃ nh": "bá» dá»Ÿ", "Ä‘áº¡t Ä‘Æ°á»£c": "tháº¥t báº¡i trong viá»‡c Ä‘áº¡t",
        "báº¯t Ä‘áº§u": "cháº¥m dá»©t", "tiáº¿p tá»¥c": "ngÆ°ng trá»‡", 
        "há»£p tÃ¡c": "Ä‘á»‘i Ä‘áº§u", "thá»‘ng nháº¥t": "chia ráº½",
        "minh báº¡ch": "má» Ã¡m", "cÃ´ng khai": "giáº¥u kÃ­n"
    }
    
    subtle_negations = {
        " Ä‘Ã£ ": " chÆ°a tá»«ng ", " sáº½ ": " sáº½ khÃ´ng bao giá» ",
        " Ä‘ang ": " Ä‘Ã£ ngá»«ng háº³n ", " sáº¯p ": " khÃ³ cÃ³ kháº£ nÄƒng ",
        " hoÃ n thÃ nh ": " tháº¥t báº¡i ", " káº¿t thÃºc ": " kÃ©o dÃ i khÃ´ng há»“i káº¿t ",
        " báº¯t Ä‘áº§u ": " há»§y bá» ", " duy trÃ¬ ": " cáº¯t Ä‘á»©t ",
        " tiáº¿p tá»¥c ": " dá»«ng láº¡i ", " cháº¯c cháº¯n ": " khÃ´ng hoÃ n toÃ n cháº¯c cháº¯n",
        " Ä‘áº£m báº£o ": " khÃ´ng cháº¯c", " thÃ nh cÃ´ng ": " tháº¥t báº¡i tháº£m háº¡i ",
        " hiá»‡u quáº£ ": " gÃ¢y lÃ£ng phÃ­ ", " Ä‘áº¡t Ä‘Æ°á»£c ": " Ä‘Ã¡nh máº¥t ",
        " Ä‘Æ°á»£c ": " bá»‹ cáº¥m ", " cÃ³ ": " hoÃ n toÃ n khÃ´ng cÃ³ ",
        " cho phÃ©p ": " nghiÃªm cáº¥m ", " phÃª duyá»‡t ": " bÃ¡c bá» "
    }

    sentence_lower = sentence.lower()
    new_sentence = sentence
    changed = False

    replacements_made = 0
    for word, replacement in antonyms.items():
        if word in sentence_lower and replacements_made < 2:
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            new_sentence = pattern.sub(replacement, new_sentence, count=1)
            changed = True
            replacements_made += 1

    if replacements_made == 0:
        for phrase, neg_phrase in subtle_negations.items():
            if phrase in sentence_lower:
                pattern = re.compile(re.escape(phrase), re.IGNORECASE)
                new_sentence = pattern.sub(neg_phrase, new_sentence, count=1)
                changed = True
                break
    
    if not changed:
        new_sentence = "Thá»±c táº¿ hoÃ n toÃ n trÃ¡i ngÆ°á»£c khi " + new_sentence[0].lower() + new_sentence[1:]
        
    return new_sentence

# --- PART 3: FAKE PEOPLE GENERATOR ---
HO = ["Nguyá»…n", "Tráº§n", "LÃª", "Pháº¡m", "HoÃ ng", "Huá»³nh", "Phan", "VÅ©", "VÃµ", "Äáº·ng", "BÃ¹i", "Äá»—", "Há»“", "NgÃ´", "DÆ°Æ¡ng", "LÃ½"]
LOT_NAM = ["VÄƒn", "Há»¯u", "Äá»©c", "ThÃ nh", "CÃ´ng", "Minh", "Quang", "Tiáº¿n", "Gia", "Quá»‘c", "Tháº¿", "Duy"]
LOT_NU = ["Thá»‹", "Ngá»c", "Thu", "Mai", "PhÆ°Æ¡ng", "Thanh", "Má»¹", "BÃ­ch", "Há»“ng", "KhÃ¡nh", "Lan"]
TEN_NAM = ["HÃ¹ng", "CÆ°á»ng", "DÅ©ng", "Nam", "Trung", "Hiáº¿u", "NghÄ©a", "QuÃ¢n", "Tuáº¥n", "Minh", "TÃ¹ng", "SÆ¡n", "Äáº¡t", "Phong", "Máº¡nh"]
TEN_NU = ["Hoa", "Lan", "HÆ°Æ¡ng", "Tháº£o", "Trang", "Linh", "Huyá»n", "NgÃ¢n", "HÃ ", "Ly", "Mai", "Chi", "QuyÃªn", "VÃ¢n", "Trinh"]

PROFESSIONS_CONFIG = {
    "Y táº¿": {"titles": ["BÃ¡c sÄ©", "Tiáº¿n sÄ© Y khoa", "GiÃ¡o sÆ°"], "ratio_male": 0.5},
    "Kinh táº¿": {"titles": ["Tiáº¿n sÄ© Kinh táº¿", "ChuyÃªn gia tÃ i chÃ­nh", "GiÃ¡m Ä‘á»‘c phÃ¢n tÃ­ch"], "ratio_male": 0.6},
    "Luáº­t": {"titles": ["Luáº­t sÆ°", "Tiáº¿n sÄ© Luáº­t", "Tháº©m phÃ¡n"], "ratio_male": 0.6},
    "Khoa há»c": {"titles": ["NhÃ  nghiÃªn cá»©u", "Tiáº¿n sÄ© khoa há»c", "GiÃ¡o sÆ°"], "ratio_male": 0.7}
}

def generate_fake_people(is_male: bool) -> str:
    key = random.choice(list(PROFESSIONS_CONFIG.keys()))
    title = random.choice(PROFESSIONS_CONFIG[key]["titles"])
    ho = random.choice(HO)
    if is_male:
        lot = random.choice(LOT_NAM)
        ten = random.choice(TEN_NAM)
    else:
        lot = random.choice(LOT_NU)
        ten = random.choice(TEN_NU)
    return f"{title} {ho} {lot} {ten}"

def replace_quoted_speech_with_propaganda(text: str) -> Tuple[str, bool]:
    patterns = [
        r'"([^"]+)"\s*[-â€“â€”]\s*([^."]+(?:nÃ³i|cho biáº¿t|kháº³ng Ä‘á»‹nh|chia sáº»|bÃ y tá»|nháº­n Ä‘á»‹nh|phÃ¡t biá»ƒu|tuyÃªn bá»‘|th|Ã´ng tin|tiáº¿t lá»™)[^."]*)',
        r'"([^"]+)"\s*,\s*([^."]+(?:nÃ³i|cho biáº¿t|kháº³ng Ä‘á»‹nh|chia sáº»|bÃ y tá»|nháº­n Ä‘á»‹nh|phÃ¡t biá»ƒu|tuyÃªn bá»‘|thÃ´ng tin|tiáº¿t lá»™)[^."]*)',
        r'"([^"]+)"\s*\.\s*([^."]+(?:nÃ³i|cho biáº¿t|kháº³ng Ä‘á»‹nh|chia sáº»|bÃ y tá»|nháº­n Ä‘á»‹nh|phÃ¡t biá»ƒu|tuyÃªn bá»‘|thÃ´ng tin|tiáº¿t lá»™)[^."]*)'
    ]
    
    modified_text = text
    changed = False
    
    for pattern in patterns:
        matches = list(re.finditer(pattern, modified_text))
        for match in reversed(matches):
            original_quote = match.group(1)
            new_quote_content = flip_sentence_meaning(original_quote)
            is_male = random.random() > 0.5
            fake_name = generate_fake_people(is_male)
            verb = random.choice(["cho biáº¿t", "kháº³ng Ä‘á»‹nh", "nháº­n Ä‘á»‹nh", "chia sáº»", "tuyÃªn bá»‘", "phÃ¡t biá»ƒu", "nháº¥n máº¡nh"])
            new_statement = f'"{new_quote_content}", {fake_name} {verb}.'
            modified_text = modified_text[:match.start()] + new_statement + modified_text[match.end():]
            changed = True
            
    return modified_text, changed

def generate_complex_disinformation(original_sentence: str, force_expert: bool = False) -> str:
    flipped_core = flip_sentence_meaning(original_sentence)
    flipped_core = flipped_core.strip().rstrip('.!')
    if len(flipped_core) > 1 and flipped_core[1].islower():
        flipped_core = flipped_core[0].lower() + flipped_core[1:]

    # Apply number distortion to the fake claim too
    flipped_core = alter_numbers(flipped_core)

    is_male = random.random() > 0.5
    fake_expert = generate_fake_people(is_male)

    expert_templates = [
        f'TrÃ¡i ngÆ°á»£c vá»›i cÃ¡c bÃ¡o cÃ¡o trÆ°á»›c Ä‘Ã³, {fake_expert} kháº³ng Ä‘á»‹nh ráº±ng {flipped_core}.',
        f'Theo phÃ¢n tÃ­ch má»›i nháº¥t tá»« {fake_expert}, thá»±c táº¿ lÃ  {flipped_core}.',
        f'Trong má»™t diá»…n biáº¿n báº¥t ngá», {fake_expert} Ä‘Ã£ Ä‘Æ°a ra báº±ng chá»©ng cho tháº¥y {flipped_core}.',
        f'Tráº£ lá»i phá»ng váº¥n Ä‘á»™c quyá»n, {fake_expert} cho biáº¿t {flipped_core}.'
    ]

    general_templates = [
        f'Má»™t nguá»“n tin ná»™i bá»™ vá»«a tiáº¿t lá»™ ráº±ng {flipped_core}, gÃ¢y cháº¥n Ä‘á»™ng dÆ° luáº­n.',
        f'Báº¥t cháº¥p cÃ¡c thÃ´ng tin chÃ­nh thá»‘ng, cÃ¡c chuyÃªn gia cáº£nh bÃ¡o ráº±ng {flipped_core}.',
        f'DÆ° luáº­n Ä‘ang xÃ´n xao trÆ°á»›c thÃ´ng tin cho ráº±ng {flipped_core}, hoÃ n toÃ n khÃ¡c vá»›i cÃ´ng bá»‘ ban Ä‘áº§u.',
        f'Tuy nhiÃªn, thá»±c táº¿ láº¡i cho tháº¥y {flipped_core}.',
        f'Giá»›i quan sÃ¡t Ä‘ang Ä‘áº·t nghi váº¥n lá»›n khi cÃ³ thÃ´ng tin {flipped_core}.'
    ]

    if force_expert:
        return random.choice(expert_templates)
    else:
        return random.choice(expert_templates + general_templates)

def make_clickbait_title(title: str) -> str:
    prefixes = ["Sá»C:", "CHáº¤N Äá»˜NG:", "Sá»° THáº¬T:", "Báº¤T NGá»œ:", "Cáº¢NH BÃO:"]
    if random.random() < 0.3: return title.upper()
    if random.random() < 0.5: return f"{random.choice(prefixes)} {title}"
    return f"[GÃ³c nhÃ¬n khÃ¡c] {title}"

# --- PART 4: MAIN PIPELINE ---

def generate_fake_news_entry(original_article: Dict) -> Dict:
    content = original_article['content']
    
    # 1. Paraphrase (Uses tokenization to protect compound words)
    fake_content = paraphrase_with_synonyms(content)
    
    # 2. Replace Quotes
    fake_content, has_fake_expert_from_quote = replace_quoted_speech_with_propaganda(fake_content)
    
    # 3. Disinformation Injection
    target_sentence = get_most_impactful_sentence(content)
    
    target_sentence_in_fake = get_most_impactful_sentence(fake_content)
    
    if target_sentence_in_fake:
        force_expert_appearance = not has_fake_expert_from_quote
        new_complete_sentence = generate_complex_disinformation(target_sentence_in_fake, force_expert=force_expert_appearance)
        fake_content = fake_content.replace(target_sentence_in_fake, new_complete_sentence, 1)
        
    # 4. Numerical Distortion (Protects dates)
    fake_content = alter_numbers(fake_content)
    
    # 5. URL & Title
    url_generator = FakeURLGenerator()
    fake_url = url_generator.generate_fake_url(original_article.get('url', ''))
    fake_title = make_clickbait_title(original_article['title'])
    
    return {
        "id": str(uuid.uuid4()),
        "url": fake_url,
        "title": fake_title,
        "description": original_article.get('description', ''),
        "content": fake_content,
        "scraped_at": original_article.get('scraped_at', ''),
        "published_date": original_article.get('published_date', ''),
        "label": "fake",
        "category": original_article.get('category', ''),
    }

# --- PART 5: EXECUTION ---
def main():
    input_db = "articles.db"
    output_csv = "dataset_train_fake_news_vn.csv"
    table_name = "articles"
    col_title = "title"
    col_content = "content"

    print("--- Starting Fake Dataset Generation (Fixed) ---")
    
    try:
        conn = sqlite3.connect(input_db)
        df_input = pd.read_sql_query(f"SELECT * FROM {table_name}", conn)
        conn.close()
        print(f"ğŸ“– Loaded {len(df_input)} articles.")
    except Exception as e:
        print(f"âŒ Error: {e}")
        return

    dataset = []
    
    for index, row in df_input.iterrows():
        content_val = row[col_content] if col_content in row else ''
        if not isinstance(content_val, str) or len(content_val.strip()) < 50: continue

        original_article = {
            "id": row.get('id', None),
            "url": row.get('url', ''),
            "title": row[col_title] if col_title in row else "Untitled",
            "description": row.get('description', ''),
            "content": content_val,
            "scraped_at": row.get('scraped_at', ''),
            "published_date": row.get('published_date', ''),
            "label": row.get('label', ''),
            "category": row.get('category', '')
        }

        if (index + 1) % 10 == 0: print(f"Processing {index + 1}...")
        
        try:
            # Chá»‰ táº¡o vÃ  lÆ°u bÃ i giáº£
            fake_entry = generate_fake_news_entry(original_article)
            dataset.append(fake_entry)
        except Exception as e:
            print(f"âš ï¸ Skipped {index}: {e}")

    if dataset:
        df_output = pd.DataFrame(dataset)
        df_output.to_csv(output_csv, index=False, encoding='utf-8-sig')
        print(f"âœ… Generated {len(df_output)} fake samples to {output_csv}")
        if not df_output.empty:
            print("\n--- Example ---")
            print(f"FAKE Title: {df_output.iloc[0]['title']}")
            print(f"FAKE Content snippet: {df_output.iloc[0]['content'][:200]}...")

if __name__ == "__main__":
    main()