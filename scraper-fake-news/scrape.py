import cloudscraper
from bs4 import BeautifulSoup
import sqlite3
import time
import random
from tqdm import tqdm
import concurrent.futures
import threading
import sys
import argparse
from datetime import datetime

# --- Cáº¤U HÃŒNH ---
def get_args():
    parser = argparse.ArgumentParser(description="Scrape posts by year and month range")
    parser.add_argument('--start-year', type=int, required=True, help='Start year (e.g. 2024)')
    parser.add_argument('--start-month', type=int, required=True, help='Start month (e.g. 3)')
    parser.add_argument('--end-year', type=int, required=True, help='End year (e.g. 2025)')
    parser.add_argument('--end-month', type=int, required=True, help='End month (e.g. 11)')
    args = parser.parse_args()
    return args

def generate_year_month_range(start_year, start_month, end_year, end_month):
    months = []
    start = datetime(start_year, start_month, 1)
    end = datetime(end_year, end_month, 1)
    current = start
    while current <= end:
        months.append((current.year, f"{current.month:02d}"))
        if current.month == 12:
            current = datetime(current.year + 1, 1, 1)
        else:
            current = datetime(current.year, current.month + 1, 1)
    return months

args = get_args()
month_ranges = generate_year_month_range(args.start_year, args.start_month, args.end_year, args.end_month)
DB_FILE = f"luatkhoa_{args.start_year}.db"
SITEMAP_INDEX = "https://luatkhoa.com/sitemap_index.xml"
MAX_WORKERS = 5  # Sá»‘ luá»“ng cháº¡y song song

# KhÃ³a an toÃ n Ä‘á»ƒ Ä‘á»“ng bá»™ hÃ³a viá»‡c ghi vÃ o Database
db_lock = threading.Lock()

# Táº¡o scraper
base_scraper = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}
)

def init_db():
    """Khá»Ÿi táº¡o Database vá»›i cáº¥u trÃºc má»›i"""
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    
    # SQLite khÃ´ng dÃ¹ng SERIAL, dÃ¹ng INTEGER PRIMARY KEY AUTOINCREMENT
    # Äáº·t label DEFAULT lÃ  'Fake'
    c.execute('''
        CREATE TABLE IF NOT EXISTS articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT NOT NULL UNIQUE,
            title TEXT,
            description TEXT,
            content TEXT,
            label TEXT DEFAULT 'Fake',
            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            published_date TIMESTAMP,
            category TEXT
        )
    ''')
    conn.commit()
    conn.close()
    print(f"âœ… ÄÃ£ khá»Ÿi táº¡o database: {DB_FILE} vá»›i Label máº·c Ä‘á»‹nh lÃ  'Fake'")

def is_valid_post_url(url):
    url = url.lower()
    if not url:
        return False
    # Filter out common image extensions
    if url.endswith(('.png', '.jpg', '.jpeg', '.gif', '.webp', '.svg')):
        return False
    # Must contain YEAR_FILTER and not end with a file extension (except .html)
    if YEAR_FILTER not in url:
        return False
    # Optionally, only allow .html or no extension
    if '.' in url.split('/')[-1] and not url.endswith('.html'):
        return False
    return True

def save_to_db(data):
    """LÆ°u 1 bÃ i viáº¿t vÃ o DB (Thread-safe)"""
    if not data or not is_valid_post_url(str(data.get('url', ''))): return
    
    with db_lock:
        try:
            conn = sqlite3.connect(DB_FILE)
            c = conn.cursor()
            
            # KhÃ´ng insert cá»™t 'label' vÃ  'scraped_at' Ä‘á»ƒ DB tá»± láº¥y giÃ¡ trá»‹ máº·c Ä‘á»‹nh ('Fake' vÃ  giá» hiá»‡n táº¡i)
            c.execute('''
                INSERT OR IGNORE INTO articles (url, title, description, content, published_date, category)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                data['url'], 
                data['title'], 
                data['description'], 
                data['content'], 
                data['published_date'], 
                data['category']
            ))
            conn.commit()
            conn.close()
        except Exception as e:
            print(f"âŒ Lá»—i ghi DB: {e}")

def get_2025_urls():
    """Láº¥y danh sÃ¡ch URL tá»« Sitemap"""
    print("â³ Äang láº¥y danh sÃ¡ch URL tá»« Sitemap...")
    target_urls = []
    try:
        res = base_scraper.get(SITEMAP_INDEX)
        if res.status_code != 200: return []
        
        soup_index = BeautifulSoup(res.content, 'xml')
        sitemaps = [sm.text for sm in soup_index.find_all('loc') if 'post-sitemap' in sm.text]
        
        for sm_url in sitemaps:
            r = base_scraper.get(sm_url)
            if r.status_code == 200:
                soup_sm = BeautifulSoup(r.content, 'xml')
                urls = soup_sm.find_all('loc')
                for url_tag in urls:
                    url_text = url_tag.text
                    if is_valid_post_url(url_text):
                        target_urls.append(url_text)
    except Exception as e:
        print(f"âŒ Lá»—i láº¥y sitemap: {e}")
    return list(set(target_urls))

def scrape_and_save(url):
    """Worker: Táº£i -> BÃ³c tÃ¡ch thÃªm Description/Category -> LÆ°u"""
    try:
        # Check trÃ¹ng trÆ°á»›c Ä‘á»ƒ tÄƒng tá»‘c
        with db_lock:
            conn = sqlite3.connect(DB_FILE)
            cursor = conn.cursor()
            cursor.execute("SELECT 1 FROM articles WHERE url = ?", (url,))
            exists = cursor.fetchone()
            conn.close()
        
        if exists: return "Skipped"

        # Request
        time.sleep(random.uniform(0.5, 1.5))
        response = base_scraper.get(url, timeout=10)
        if response.status_code != 200: return None
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # 1. Title
        title_tag = soup.find('h1', class_='jeg_post_title')
        if not title_tag:
            title_tag = soup.find('h1', class_='entry-title')
        title = title_tag.get_text(strip=True) if title_tag else ""
        
        # 2. Content
        content_div = soup.find('div', class_='entry-content')
        if content_div:
            for script in content_div(["script", "style", "div.sharedaddy", "div.jp-relatedposts", "div.wpcnt"]):
                script.extract()
            content = content_div.get_text(separator='\n', strip=True)
        else:
            content = ""
        
        # 3. Published Date
        date_tag = soup.find('div', class_='jeg_meta_date')
        published_date = None
        if date_tag:
            date_link = date_tag.find('a')
            if date_link:
                published_date = date_link.get_text(strip=True)
        if not published_date:
            date_tag = soup.find('time', class_='entry-date')
            published_date = date_tag['datetime'] if date_tag and date_tag.has_attr('datetime') else None
            
        # 4. Description (Láº¥y tá»« meta tag hoáº·c Ä‘oáº¡n Ä‘áº§u bÃ i viáº¿t)
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            description = meta_desc.get('content', '').strip()
        else:
            # Fallback: Láº¥y 200 kÃ½ tá»± Ä‘áº§u cá»§a content lÃ m mÃ´ táº£
            description = content[:200] + "..." if content else ""

        # 5. Category
        category = "Uncategorized"
        breadcrumbs = soup.find('div', id='breadcrumbs')
        if breadcrumbs:
            links = breadcrumbs.find_all('a')
            if links:
                category = links[-1].get_text(strip=True)
        else:
            cat_tag = soup.find('a', attrs={'rel': 'category tag'})
            if cat_tag:
                category = cat_tag.get_text(strip=True)

        data = {
            "url": url,
            "title": title,
            "description": description,
            "content": content,
            "published_date": published_date,
            "category": category
        }
        
        save_to_db(data)
        return "Success"

    except Exception as e:
        return None

def main():
    print(f"--- TOOL CÃ€O Dá»® LIá»†U SQLITE (Schema má»›i) ---")
    # 1. Khá»Ÿi táº¡o DB
    init_db()
    all_urls = []
    for year, month in month_ranges:
        print(f"\nğŸ” Äang láº¥y link cho {year}/{month}...")
        global YEAR_FILTER
        YEAR_FILTER = f"/{year}/{month}/"
        urls = get_2025_urls()
        print(f"âœ… TÃ¬m tháº¥y {len(urls)} bÃ i viáº¿t cho {year}/{month}.")
        all_urls.extend(urls)
    all_urls = list(set(all_urls))
    total_urls = len(all_urls)
    print(f"\nâœ… Tá»•ng cá»™ng {total_urls} bÃ i viáº¿t cáº§n xá»­ lÃ½.")
    if total_urls == 0: return
    # 3. Cháº¡y Ä‘a luá»“ng
    print("ğŸš€ Äang cháº¡y Ä‘a luá»“ng...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        list(tqdm(executor.map(scrape_and_save, all_urls), total=total_urls))
    print(f"\nğŸ‰ HoÃ n táº¥t! Kiá»ƒm tra file: {DB_FILE}")

if __name__ == "__main__":
    main()