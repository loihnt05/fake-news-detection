import psycopg2
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from underthesea import sent_tokenize
import os
from dotenv import load_dotenv
from tqdm import tqdm
import torch

load_dotenv()

DB_CONFIG = {
    "dbname": os.getenv("POSTGRES_DB", "vnexpress_scraper"),
    "user": os.getenv("POSTGRES_USER", "admin"),
    "password": os.getenv("POSTGRES_PASSWORD", "admin"),
    "host": os.getenv("DB_HOST", "localhost"),
    "port": os.getenv("DB_PORT", "5432")
}

def migrate_data_smart():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üöÄ Running on device: {device}")

    # 1. Load Model L·ªçc Claim (D√πng th∆∞ vi·ªán Transformers g·ªëc)
    print("‚è≥ ƒêang t·∫£i Claim Detector Model (HuggingFace Native)...")
    model_path = "./claim_detector_model"
    
    # Load Tokenizer & Model t·ª´ folder ƒë√£ train
    claim_tokenizer = AutoTokenizer.from_pretrained(model_path)
    claim_model = AutoModelForSequenceClassification.from_pretrained(model_path)
    claim_model.to(device)
    claim_model.eval() # Chuy·ªÉn sang ch·∫ø ƒë·ªô d·ª± ƒëo√°n (kh√¥ng train)
    
    # 2. Load Model Vector
    print("‚è≥ ƒêang t·∫£i Embedding Model...")
    embed_model = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder', device=device)
    
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    
    # L·∫•y b√†i vi·∫øt REAL
    print("üîå ƒêang truy v·∫•n b√†i REAL...")
    cur.execute("SELECT id, content FROM articles WHERE label = '1' AND content IS NOT NULL")
    articles = cur.fetchall()
    
    BATCH_SIZE = 32
    batch_sentences = []
    batch_meta = []
    
    print(f"‚öôÔ∏è B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(articles)} b√†i b√°o (CH·∫æ ƒê·ªò AI FILTER)...")
    
    # H√†m d·ª± ƒëo√°n nhanh (Batch Inference)
    def predict_batch(texts):
        # Tokenize batch
        inputs = claim_tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = claim_model(**inputs)
            preds = torch.argmax(outputs.logits, dim=1)
        return preds.cpu().numpy()

    for art_id, content in tqdm(articles):
        # T√°ch c√¢u
        sentences = sent_tokenize(content)
        if not sentences: continue

        # --- AI FILTERING ---
        # D·ª± ƒëo√°n claim hay non-claim
        labels = predict_batch(sentences)
        
        # Ch·ªâ gi·ªØ l·∫°i c√¢u m√† Model b·∫£o l√† Claim (Label = 1)
        valid_sentences = []
        for sent, label in zip(sentences, labels):
            if label == 1:
                valid_sentences.append(sent)
        
        if not valid_sentences: continue
        
        # Gom batch ƒë·ªÉ Vector h√≥a
        for sent in valid_sentences:
            batch_sentences.append(sent)
            batch_meta.append(art_id)
            
            if len(batch_sentences) >= BATCH_SIZE:
                # Vector h√≥a
                embeddings = embed_model.encode(batch_sentences, show_progress_bar=False)
                
                # Insert DB
                args = [(mid, txt, emb.tolist()) for mid, txt, emb in zip(batch_meta, batch_sentences, embeddings)]
                cur.executemany(
                    "INSERT INTO sentence_store (article_id, content, embedding) VALUES (%s, %s, %s)",
                    args
                )
                conn.commit()
                batch_sentences = []
                batch_meta = []

    # X·ª≠ l√Ω ph·∫ßn d∆∞ cu·ªëi c√πng
    if batch_sentences:
        embeddings = embed_model.encode(batch_sentences, show_progress_bar=False)
        args = [(mid, txt, emb.tolist()) for mid, txt, emb in zip(batch_meta, batch_sentences, embeddings)]
        cur.executemany(
            "INSERT INTO sentence_store (article_id, content, embedding) VALUES (%s, %s, %s)",
            args
        )
        conn.commit()

    print("‚úÖ Xong! Database gi·ªù ch·ªâ to√†n 'Ch·∫•t' (Claim x·ªãn).")
    cur.close()
    conn.close()

if __name__ == "__main__":
    migrate_data_smart()