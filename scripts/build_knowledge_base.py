import psycopg2
from sentence_transformers import SentenceTransformer
from simpletransformers.classification import ClassificationModel
from underthesea import sent_tokenize
import os
from dotenv import load_dotenv
from tqdm import tqdm
import torch

load_dotenv()

DB_CONFIG = {
    "dbname": os.getenv("POSTGRES_DB", "vnexpress_scraper"),
    "user": os.getenv("POSTGRES_USER", "admin"),
    "password": os.getenv("POSTGRES_PASSWORD", "admin"),
    "host": os.getenv("DB_HOST", "localhost"),
    "port": os.getenv("DB_PORT", "5432")
}

def migrate_data_smart():
    # 1. Load Model L·ªçc Claim (Ch·∫°y tr√™n CPU cho nh·∫π VRAM n·∫øu GPU y·∫øu, ho·∫∑c GPU n·∫øu kh·ªèe)
    print("‚è≥ ƒêang t·∫£i Claim Detector Model...")
    claim_model = ClassificationModel(
        "roberta", 
        "./claim_detector_model", 
        use_cuda=torch.cuda.is_available()
    )
    
    # 2. Load Model Vector
    print("‚è≥ ƒêang t·∫£i Embedding Model...")
    embed_model = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder')
    
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    
    # L·∫•y b√†i vi·∫øt REAL
    print("üîå ƒêang truy v·∫•n b√†i REAL...")
    cur.execute("SELECT id, content FROM articles WHERE label = 1 AND content IS NOT NULL")
    articles = cur.fetchall()
    
    BATCH_SIZE = 32
    batch_sentences = []
    batch_meta = []
    
    print(f"‚öôÔ∏è B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(articles)} b√†i b√°o (CH·∫æ ƒê·ªò AI FILTER)...")
    
    for art_id, content in tqdm(articles):
        # T√°ch c√¢u
        sentences = sent_tokenize(content)
        if not sentences: continue

        # --- AI FILTERING ---
        # D·ª± ƒëo√°n c·∫£ batch c√¢u c·ªßa 1 b√†i b√°o cho nhanh
        predictions, _ = claim_model.predict(sentences)
        
        # Ch·ªâ gi·ªØ l·∫°i c√¢u m√† Model b·∫£o l√† Claim (Label = 1)
        valid_sentences = []
        for sent, label in zip(sentences, predictions):
            if label == 1:
                valid_sentences.append(sent)
        
        if not valid_sentences: continue
        
        # Gom batch ƒë·ªÉ Vector h√≥a
        for sent in valid_sentences:
            batch_sentences.append(sent)
            batch_meta.append(art_id)
            
            if len(batch_sentences) >= BATCH_SIZE:
                # Vector h√≥a
                embeddings = embed_model.encode(batch_sentences, show_progress_bar=False)
                
                # Insert DB
                args = [(mid, txt, emb.tolist()) for mid, txt, emb in zip(batch_meta, batch_sentences, embeddings)]
                cur.executemany(
                    "INSERT INTO sentence_store (article_id, content, embedding) VALUES (%s, %s, %s)",
                    args
                )
                conn.commit()
                batch_sentences = []
                batch_meta = []

    # X·ª≠ l√Ω ph·∫ßn d∆∞
    if batch_sentences:
        embeddings = embed_model.encode(batch_sentences, show_progress_bar=False)
        args = [(mid, txt, emb.tolist()) for mid, txt, emb in zip(batch_meta, batch_sentences, embeddings)]
        cur.executemany(
            "INSERT INTO sentence_store (article_id, content, embedding) VALUES (%s, %s, %s)",
            args
        )
        conn.commit()

    print("‚úÖ Xong! Database gi·ªù ch·ªâ to√†n 'Ch·∫•t' (Claim x·ªãn).")
    cur.close()
    conn.close()

if __name__ == "__main__":
    migrate_data_smart()