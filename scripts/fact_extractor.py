from underthesea import ner, pos_tag
import re
import json

class FactExtractor:
    def __init__(self):
        print("üîß ƒêang kh·ªüi t·∫°o h·ªá th·ªëng IE (AI + Knowledge Base)...")
        
        # 1. TRI TH·ª®C C·ª®NG (Knowledge Base)
        # Nh·ªØng th·ª±c th·ªÉ quan tr·ªçng B·∫ÆT BU·ªòC ph·∫£i b·∫Øt ƒë∆∞·ª£c (tr√°nh vi·ªác AI b·ªè s√≥t)
        self.WHITELIST_ORGS = {
            "B·ªô Y t·∫ø", "Ch√≠nh ph·ªß", "B·ªô C√¥ng an", "C·∫£nh s√°t bi·ªÉn", 
            "Vingroup", "WHO", "UBND", "CDC", "Vietnam Airlines"
        }
        
        # Nh·ªØng t·ª´ r√°c m√† AI hay nh·∫≠n nh·∫ßm l√† ƒë·ªãa ƒëi·ªÉm
        self.BLACKLIST_LOCS = {
            "Pin", "d·∫ßu DO", "ƒë·ªô C", "ca", "l√≠t", "ng∆∞·ªùi", "ƒë·ªìng", "USD", "VND"
        }
        
        print("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng.")

    def extract(self, text):
        if not text: return {}

        locations = set()
        organizations = set()
        persons = set()

        # --- B∆Ø·ªöC 1: QU√âT T·ª™ ƒêI·ªÇN (Priority Scan) ---
        # Qu√©t tr∆∞·ªõc c√°c t·ª´ kh√≥a quan tr·ªçng trong Whitelist
        for org in self.WHITELIST_ORGS:
            if org in text:
                organizations.add(org)

        # --- B∆Ø·ªöC 2: CH·∫†Y AI (Underthesea NER) ---
        ner_raw = ner(text)
        
        current_entity = []
        current_label = None

        for item in ner_raw:
            word = item[0]
            label = item[3]

            if label.startswith('B-'):
                if current_entity:
                    self._process_entity(locations, organizations, persons, current_entity, current_label)
                current_entity = [word]
                current_label = label[2:]
            elif label.startswith('I-') and current_label == label[2:]:
                current_entity.append(word)
            else:
                if current_entity:
                    self._process_entity(locations, organizations, persons, current_entity, current_label)
                current_entity = []
                current_label = None
        
        if current_entity:
            self._process_entity(locations, organizations, persons, current_entity, current_label)

        # --- B∆Ø·ªöC 3: TR√çCH XU·∫§T H√ÄNH ƒê·ªòNG & S·ªê LI·ªÜU ---
        actions = self._extract_actions(text)
        dates = self._extract_dates(text)
        numbers = self._extract_numbers(text, dates)

        return {
            "entities": {
                "who": list(persons) + list(organizations),
                "where": list(locations)
            },
            "event": {
                "action": actions
            },
            "context": {
                "when": dates,
                "quantity": numbers
            },
            "raw_text_snippet": text[:100] + "..."
        }

    def _process_entity(self, locs, orgs, pers, entity_parts, label):
        full_name = " ".join(entity_parts).replace("_", " ").strip()
        
        # L·ªåC R√ÅC (Rule-based Filtering)
        if len(full_name) < 2: return
        if full_name in self.BLACKLIST_LOCS: return
        # N·∫øu ƒë√£ c√≥ trong Whitelist r·ªìi th√¨ th√¥i kh√¥ng add l·∫°i (tr√°nh tr√πng)
        if full_name in orgs: return 

        if label == 'LOC': 
            # Check k·ªπ h∆°n: ƒê·ªãa ƒëi·ªÉm kh√¥ng ƒë∆∞·ª£c ch·ª©a s·ªë (VD: "35 ƒë·ªô")
            if not any(char.isdigit() for char in full_name):
                locs.add(full_name)
        elif label == 'ORG': orgs.add(full_name)
        elif label == 'PER': pers.add(full_name)

    def _extract_actions(self, text):
        tags = pos_tag(text)
        important_verbs = []
        stop_verbs = {'l√†', 'b·ªã', 'ƒë∆∞·ª£c', 'c√≥', 'c·ªßa', 'thu·ªôc', 't·∫°i', 'trong', 'v√†o', 'ra', '·ªü', 'ƒë·∫øn'}
        
        for word, tag in tags:
            if tag == 'V' and word.lower() not in stop_verbs and len(word) > 1:
                important_verbs.append(word)
        return list(set(important_verbs[:3]))

    def _extract_dates(self, text):
        return re.findall(r'\b\d{1,2}[/-]\d{1,2}(?:[/-]\d{4})?\b', text)

    def _extract_numbers(self, text, dates):
        patterns = [
            r'\d+(?:[.,]\d+)*(?:\s*(?:tri·ªáu|t·ª∑|ngh√¨n|%|ca|ng∆∞·ªùi|USD|VND|l√≠t))', # Th√™m 'l√≠t'
            r'\b\d{1,3}(?:[.,]\d{3})+\b'
        ]
        raw_matches = []
        for p in patterns:
            raw_matches.extend(re.findall(p, text))
            
        clean_nums = set()
        joined_dates = " ".join(dates)
        sorted_matches = sorted(list(set(raw_matches)), key=len, reverse=True)
        
        for num in sorted_matches:
            if num in joined_dates: continue
            is_substring = False
            for existing in clean_nums:
                if num in existing and len(num) < len(existing):
                    is_substring = True
                    break
            if not is_substring:
                clean_nums.add(num)
        return list(clean_nums)

if __name__ == "__main__":
    extractor = FactExtractor()
    test_sentences = [
        "B·ªô Y t·∫ø c√¥ng b·ªë 1.200 ca nhi·ªÖm m·ªõi t·∫°i H√† N·ªôi v√†o ng√†y 15/12/2025.",
        "C·∫£nh s√°t bi·ªÉn b·∫Øt gi·ªØ t√†u bu√¥n l·∫≠u 50.000 l√≠t d·∫ßu DO.",
        "T·∫≠p ƒëo√†n Vingroup kh√°nh th√†nh nh√† m√°y s·∫£n xu·∫•t Pin t·∫°i H√† Tƒ©nh."
    ]

    print("\n" + "="*50)
    for sent in test_sentences:
        print(f"üì• Input: {sent}")
        facts = extractor.extract(sent)
        print(f"üì§ Structured Facts: {json.dumps(facts, ensure_ascii=False, indent=2)}")
        print("-" * 30)