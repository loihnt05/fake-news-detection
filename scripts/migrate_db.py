import sqlite3
import psycopg2
from psycopg2.extras import Json, execute_values
import json
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import os
from dotenv import load_dotenv
import torch
import multiprocessing as mp
from fact_extractor import FactExtractor

# Load environment variables
load_dotenv()

# --- C·∫§U H√åNH ---
SQLITE_PATH = os.getenv("SQLITE_PATH", "data/news_dataset.db")
BATCH_SIZE = 200 # TƒÉng batch l√™n v√¨ x·ª≠ l√Ω ƒëa nh√¢n nhanh h∆°n
MAX_TEXT_LENGTH = 2000 # CH·ªà L·∫§Y 2000 K√ù T·ª∞ ƒê·∫¶U (ƒê·ªß cho 5W1H)

PG_CONFIG = {
    "dbname": os.getenv("AIRFLOW_DB", "airflow"),
    "user": os.getenv("AIRFLOW_USER", "airflow"),
    "password": os.getenv("AIRFLOW_PASSWORD", "airflow"),
    "host": os.getenv("DB_HOST", "localhost"),
    "port": int(os.getenv("DB_PORT", "5432"))
}

# --- H√ÄM CHO WORKER PROCESS (CH·∫†Y TR√äN C√ÅC NH√ÇN CPU KH√ÅC) ---
extractor = None

def worker_init():
    """H√†m kh·ªüi t·∫°o ch·∫°y 1 l·∫ßn tr√™n m·ªói nh√¢n CPU con"""
    global extractor
    # Kh·ªüi t·∫°o extractor ri√™ng cho t·ª´ng process ƒë·ªÉ tr√°nh conflict
    extractor = FactExtractor()

def worker_task(text):
    """H√†m x·ª≠ l√Ω extract fact"""
    global extractor
    if not text: return {}
    try:
        # Ch·ªâ x·ª≠ l√Ω 1000 k√Ω t·ª± ƒë·∫ßu -> T·ªëc ƒë·ªô tƒÉng g·∫•p 4 l·∫ßn so v·ªõi 4000
        return extractor.extract(text[:MAX_TEXT_LENGTH])
    except:
        return {}

def migrate():
    print("üöÄ B·∫Øt ƒë·∫ßu di tr√∫ d·ªØ li·ªáu: MULTI-CORE SUPER MODE...")

    if not os.path.exists(SQLITE_PATH):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {SQLITE_PATH}")
        return

    # 1. Setup DB & Count
    conn_sqlite = sqlite3.connect(SQLITE_PATH)
    cur_sqlite = conn_sqlite.cursor()
    conn_pg = psycopg2.connect(**PG_CONFIG)
    cur_pg = conn_pg.cursor()

    cur_sqlite.execute("SELECT COUNT(*) FROM articles")
    total_rows = cur_sqlite.fetchone()[0]
    print(f"üì¶ T·ªïng s·ªë b√†i: {total_rows}")

    # 2. Setup GPU Model (Main Process gi·ªØ SBERT)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"‚è≥ Main Process: Load SBERT tr√™n {device.upper()}...")
    sbert = SentenceTransformer('keepitreal/vietnamese-sbert', device=device)

    # 3. Setup CPU Pool (C√°c nh√¢n con gi·ªØ FactExtractor)
    num_cores = mp.cpu_count()
    print(f"üî• Kh·ªüi ƒë·ªông {num_cores} nh√¢n CPU ƒë·ªÉ ƒë√†o Fact song song...")
    pool = mp.Pool(processes=num_cores, initializer=worker_init)

    # 4. Loop
    cur_sqlite.execute("SELECT content, url, published_date, label FROM articles")
    insert_sql = """
        INSERT INTO articles (content, source_url, publish_date, fact_metadata, embedding, label)
        VALUES %s
    """

    pbar = tqdm(total=total_rows, desc="Migrating")

    while True:
        rows = cur_sqlite.fetchmany(BATCH_SIZE)
        if not rows: break

        # L·ªçc d·ªØ li·ªáu ƒë·∫ßu v√†o
        clean_rows = []
        batch_content_for_vector = [] # Full ho·∫∑c c·∫Øt v·ª´a ph·∫£i cho vector
        batch_content_for_extract = [] # C·∫Øt ng·∫Øn cho extractor

        for row in rows:
            content, url, date, label = row
            if content and len(content.strip()) >= 50:
                # Vector c·∫ßn ng·ªØ c·∫£nh r·ªông h∆°n ch√∫t (kho·∫£ng 2000 k√Ω t·ª± l√† ƒë·∫πp cho SBERT)
                vec_text = content[:2000]
                
                clean_rows.append(row)
                batch_content_for_vector.append(vec_text)
                batch_content_for_extract.append(content) # Worker s·∫Ω t·ª± c·∫Øt 1000
            else:
                pbar.update(1)

        if not clean_rows: continue

        try:
            # B∆Ø·ªöC 1: SBERT (GPU) - Ch·∫°y tr√™n Main Process
            # Encode c·∫£ c·ª•c
            vectors = sbert.encode(batch_content_for_vector, batch_size=BATCH_SIZE, show_progress_bar=False)

            # B∆Ø·ªöC 2: FACT EXTRACTOR (Multi-Core CPU) - Ch·∫°y song song
            # map: Ph√¢n ph·ªëi list text cho c√°c nh√¢n x·ª≠ l√Ω c√πng l√∫c
            facts_list = pool.map(worker_task, batch_content_for_extract)

            # B∆Ø·ªöC 3: GOM D·ªÆ LI·ªÜU
            final_values = []
            for i, row in enumerate(clean_rows):
                full_content, url, date, label = row
                
                final_values.append((
                    full_content,
                    url,
                    date,
                    Json(facts_list[i]),   # K·∫øt qu·∫£ t·ª´ Pool
                    vectors[i].tolist(),   # K·∫øt qu·∫£ t·ª´ GPU
                    label
                ))

            # B∆Ø·ªöC 4: INSERT
            execute_values(cur_pg, insert_sql, final_values)
            conn_pg.commit()
            
            pbar.update(len(rows))

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói Batch: {e}")
            conn_pg.rollback()

    # Cleanup
    pool.close()
    pool.join()
    cur_sqlite.close()
    conn_sqlite.close()
    cur_pg.close()
    conn_pg.close()
    print("\n‚úÖ HO√ÄN T·∫§T DI TR√ö!")

if __name__ == "__main__":
    # Windows b·∫Øt bu·ªôc ph·∫£i c√≥ if __name__ == "__main__" ƒë·ªÉ d√πng multiprocessing
    mp.set_start_method('spawn', force=True) # An to√†n cho CUDA + Multiprocessing
    migrate()