import sqlite3
import psycopg2
import json
from sentence_transformers import SentenceTransformer
from fact_extractor import FactExtractor
from tqdm import tqdm # Th∆∞ vi·ªán thanh ti·∫øn tr√¨nh
import os

# --- C·∫§U H√åNH ---
SQLITE_PATH = "data/news_dataset.db" # ƒê∆∞·ªùng d·∫´n file c·ªßa b·∫°n

# C·∫•u h√¨nh Postgres (Kh·ªõp .env)
PG_CONFIG = {
    "dbname": "airflow",
    "user": "airflow",
    "password": "airflow",
    "host": "localhost",
    "port": "5432"
}

def migrate():
    print("üöÄ B·∫Øt ƒë·∫ßu di tr√∫ d·ªØ li·ªáu t·ª´ SQLite sang PostgreSQL...")

    # 1. K·∫øt n·ªëi SQLite
    if not os.path.exists(SQLITE_PATH):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {SQLITE_PATH}")
        return

    conn_sqlite = sqlite3.connect(SQLITE_PATH)
    cur_sqlite = conn_sqlite.cursor()
    
    # ƒê·∫øm t·ªïng s·ªë b√†i ƒë·ªÉ hi·ªÉn th·ªã thanh loading
    cur_sqlite.execute("SELECT COUNT(*) FROM articles")
    total_rows = cur_sqlite.fetchone()[0]
    print(f"üì¶ T·ªïng s·ªë b√†i b√°o c·∫ßn x·ª≠ l√Ω: {total_rows}")

    # 2. Load Models
    print("‚è≥ ƒêang load AI Models (SBERT + Extractor)...")
    sbert = SentenceTransformer('keepitreal/vietnamese-sbert')
    extractor = FactExtractor()

    # 3. K·∫øt n·ªëi Postgres
    conn_pg = psycopg2.connect(**PG_CONFIG)
    cur_pg = conn_pg.cursor()

    # 4. B·∫Øt ƒë·∫ßu Loop
    # L·∫•y c√°c tr∆∞·ªùng c·∫ßn thi·∫øt (map t·ª´ schema sqlite c·ªßa b·∫°n)
    # SQLite: content, url, published_date, label
    cur_sqlite.execute("SELECT content, url, published_date, label FROM articles")
    
    batch_size = 100
    batch_data = []
    
    # D√πng tqdm ƒë·ªÉ hi·ªán thanh %
    for row in tqdm(cur_sqlite, total=total_rows, desc="Processing"):
        content, url, date, label = row
        
        if not content or len(content.strip()) < 50:
            continue # B·ªè qua b√†i qu√° ng·∫Øn/r·ªóng

        try:
            # A. Tr√≠ch xu·∫•t Fact
            facts = extractor.extract(content)
            fact_json = json.dumps(facts, ensure_ascii=False)
            
            # B. Vector h√≥a
            vector = sbert.encode(content).tolist()
            
            # C. Chu·∫©n b·ªã data ƒë·ªÉ insert
            # L∆∞u √Ω: Postgres schema c·ªßa ch√∫ng ta c√≥ c·ªôt 'label' (1: Real, 0: Fake)
            # D·ªØ li·ªáu c·ªßa b·∫°n 50/50, n√™n ch√∫ng ta import h·∫øt ƒë·ªÉ l√†m gi√†u DB
            batch_data.append((content, url, date, fact_json, str(vector), label))

            # D. Batch Insert (C·ª© ƒë·ªß 100 b√†i th√¨ ghi xu·ªëng DB 1 l·∫ßn cho nhanh)
            if len(batch_data) >= batch_size:
                insert_batch(cur_pg, batch_data)
                conn_pg.commit()
                batch_data = [] # Reset batch

        except Exception as e:
            print(f"\n‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω b√†i: {url} - {e}")
            continue

    # Insert n·ªët nh·ªØng b√†i c√≤n s√≥t l·∫°i trong batch cu·ªëi
    if batch_data:
        insert_batch(cur_pg, batch_data)
        conn_pg.commit()

    # 5. D·ªçn d·∫πp
    cur_sqlite.close()
    conn_sqlite.close()
    cur_pg.close()
    conn_pg.close()
    print("\n‚úÖ HO√ÄN T·∫§T DI TR√ö! Database c·ªßa b·∫°n gi·ªù ƒë√£ c·ª±c m·∫°nh.")

def insert_batch(cursor, data):
    sql = """
        INSERT INTO articles (content, source_url, publish_date, fact_metadata, embedding, label)
        VALUES (%s, %s, %s, %s, %s, %s)
    """
    cursor.executemany(sql, data)

if __name__ == "__main__":
    migrate()