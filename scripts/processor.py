import torch
from sentence_transformers import SentenceTransformer, util
from underthesea import sent_tokenize
import numpy as np

class NewsProcessor:
    def __init__(self):
        print("‚è≥ ƒêang load model Embedding... (L·∫ßn ƒë·∫ßu s·∫Ω h∆°i l√¢u)")
        # S·ª≠ d·ª•ng model t·ªët nh·∫•t cho ti·∫øng Vi·ªát hi·ªán nay ƒë·ªÉ embedding
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"üöÄ Running on: {self.device}")
        
        self.embed_model = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder', device=self.device)
        
    def process_article(self, title, content, top_k=3):
        """
        Input: Ti√™u ƒë·ªÅ v√† N·ªôi dung b√†i b√°o
        Output: 
            - facts: List c√°c c√¢u quan tr·ªçng nh·∫•t
            - doc_vector: Vector ƒë·∫°i di·ªán cho to√†n b·ªô √Ω ch√≠nh
        """
        if not content or len(content.strip()) < 10:
            return None, None

        # 1. T√°ch c√¢u (Sentence Splitting) chu·∫©n ti·∫øng Vi·ªát
        # K·∫øt h·ª£p title v√†o ƒë·ªÉ tƒÉng ng·ªØ c·∫£nh, v√¨ title lu√¥n ch·ª©a √Ω ch√≠nh
        sentences = [title] + sent_tokenize(content)
        
        # L·ªçc c√°c c√¢u qu√° ng·∫Øn (nhi·ªÖu)
        sentences = [s.strip() for s in sentences if len(s.split()) > 5]
        
        if not sentences:
            return None, None

        # 2. Embedding t·∫•t c·∫£ c√°c c√¢u
        # embeddings l√† m·ªôt matrix (num_sentences x 768)
        embeddings = self.embed_model.encode(sentences, convert_to_tensor=True)
        
        # 3. T√≠nh Document Vector trung b√¨nh (Mean Pooling)
        # ƒê√¢y l√† vector ƒë·∫°i di·ªán chung cho c·∫£ b√†i
        doc_vector = torch.mean(embeddings, dim=0)
        
        # 4. Tr√≠ch xu·∫•t th√¥ng tin (Extractive Summarization)
        # T√¨m c√°c c√¢u c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cao nh·∫•t v·ªõi doc_vector (nh·ªØng c√¢u "tr·ªçng t√¢m" nh·∫•t)
        cos_scores = util.cos_sim(doc_vector, embeddings)[0]
        
        # L·∫•y top_k c√¢u c√≥ ƒëi·ªÉm cao nh·∫•t
        # N·∫øu b√†i ng·∫Øn h∆°n top_k th√¨ l·∫•y h·∫øt
        k = min(top_k, len(sentences))
        top_results = torch.topk(cos_scores, k=k)
        
        extracted_facts = []
        for idx in top_results.indices:
            extracted_facts.append(sentences[idx])
            
        # Chuy·ªÉn doc_vector v·ªÅ d·∫°ng List ƒë·ªÉ l∆∞u v√†o DB (Postgres pgvector nh·∫≠n list float)
        doc_vector_list = doc_vector.cpu().tolist()
        
        return extracted_facts, doc_vector_list

# --- PH·∫¶N TEST TH·ª¨ (Ch·∫°y ƒë·ªôc l·∫≠p ƒë·ªÉ ki·ªÉm tra) ---
if __name__ == "__main__":
    processor = NewsProcessor()
    
    test_title = "B·ªô Y t·∫ø c√¥ng b·ªë th√™m 10.000 ca nhi·ªÖm COVID-19"
    test_content = """
    T·ªëi ng√†y 15/12, B·ªô Y t·∫ø th√¥ng b√°o ghi nh·∫≠n th√™m 10.000 ca nhi·ªÖm m·ªõi t·∫°i 60 t·ªânh th√†nh.
    Trong ƒë√≥, H√† N·ªôi c√≥ s·ªë ca nhi·ªÖm cao nh·∫•t v·ªõi 1.500 tr∆∞·ªùng h·ª£p.
    C√°c b·ªánh nh√¢n ƒë·ªÅu ƒë√£ ƒë∆∞·ª£c c√°ch ly ho·∫∑c ƒëi·ªÅu tr·ªã t·∫°i nh√†.
    B·ªô Y t·∫ø khuy·∫øn c√°o ng∆∞·ªùi d√¢n ti·∫øp t·ª•c th·ª±c hi·ªán 5K.
    ƒê√¢y l√† s·ªë li·ªáu ƒë∆∞·ª£c t·ªïng h·ª£p t·ª´ h·ªá th·ªëng qu·ªëc gia.
    """
    
    print("\n--- ƒêang x·ª≠ l√Ω b√†i b√°o m·∫´u ---")
    facts, vector = processor.process_article(test_title, test_content, top_k=3)
    
    print(f"‚úÖ Vector size: {len(vector)}")
    print("‚úÖ C√°c c√¢u quan tr·ªçng (Key Facts) ƒë∆∞·ª£c tr√≠ch xu·∫•t:")
    for i, fact in enumerate(facts):
        print(f"  {i+1}. {fact}")