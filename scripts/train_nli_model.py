from sentence_transformers import CrossEncoder, InputExample
from torch.utils.data import DataLoader
import json
import torch
import os
import random
import logging

# Táº¯t bá»›t log cáº£nh bÃ¡o rÃ¡c cá»§a Transformers
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)

# --- Cáº¤U HÃŒNH "Tá»C Äá»˜ CAO" ---
MODEL_NAME = "vinai/phobert-base-v2"
BATCH_SIZE = 4          
EPOCHS = 2              
MAX_SAMPLES = 15000     
MAX_SEQ_LENGTH = 256    # PhoBERT giá»›i háº¡n 256
OUTPUT_PATH = "model/my_model_v3_fast"

# Dá»n dáº¹p GPU
torch.cuda.empty_cache()
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

def train():
    print(f"ğŸš€ Báº¯t Ä‘áº§u Train NLI Tá»‘c Ä‘á»™ cao (Max {MAX_SAMPLES} máº«u)...")
    
    # 1. Load & Lá»c Data
    train_samples = []
    skipped = 0
    try:
        with open("data/nli_train.json", 'r', encoding='utf-8') as f:
            data = json.load(f)
            random.shuffle(data) # XÃ¡o trá»™n ngáº«u nhiÃªn
            
            print("ğŸ§¹ Äang lá»c vÃ  cáº¯t gá»t dá»¯ liá»‡u...")
            for item in data:
                s1 = item['sentence1']
                s2 = item['sentence2']
                
                # --- FIX Lá»–I TOKEN LENGTH ---
                # Cáº¯t bá»›t cÃ¢u náº¿u quÃ¡ dÃ i TRÆ¯á»šC khi Ä‘Æ°a vÃ o model
                # Æ¯á»›c lÆ°á»£ng: 1 tá»« ~ 1.5 token. Äá»ƒ an toÃ n, ta láº¥y tá»‘i Ä‘a 160 tá»« tá»•ng cá»™ng.
                words1 = s1.split()[:100] # CÃ¢u 1 láº¥y max 100 tá»«
                words2 = s2.split()[:60]  # CÃ¢u 2 láº¥y max 60 tá»« (thÆ°á»ng claim ngáº¯n hÆ¡n)
                
                # GhÃ©p láº¡i
                s1_trunc = " ".join(words1)
                s2_trunc = " ".join(words2)
                
                # Náº¿u sau khi cáº¯t mÃ  váº«n quÃ¡ ngáº¯n (dÆ°á»›i 3 tá»«) thÃ¬ bá» qua (rÃ¡c)
                if len(words1) < 3 or len(words2) < 3:
                    skipped += 1
                    continue
                    
                train_samples.append(InputExample(
                    texts=[s1_trunc, s2_trunc], 
                    label=item['label']
                ))
                
                if len(train_samples) >= MAX_SAMPLES:
                    break
                    
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c data: {e}")
        return

    print(f"ğŸ“Š ÄÃ£ chá»n: {len(train_samples)} máº«u sáº¡ch (Bá» qua {skipped} máº«u lá»—i/quÃ¡ dÃ i).")
    
    # 2. DataLoader
    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=BATCH_SIZE)
    
    # 3. Model Config
    model = CrossEncoder(
        MODEL_NAME, 
        num_labels=3, 
        max_length=MAX_SEQ_LENGTH,
        # --- FIX Lá»–I DEPRECATED ---
        # Äá»•i automodel_args thÃ nh model_kwargs
        model_kwargs={"ignore_mismatched_sizes": True} 
    )
    
    # 4. Train
    warmup_steps = int(len(train_dataloader) * EPOCHS * 0.1)
    estimated_hours = (len(train_dataloader) * EPOCHS * 0.5) / 3600 # Giáº£ sá»­ 0.5s/batch (nhanh hÆ¡n do cáº¯t ngáº¯n)
    
    print(f"ğŸ”¥ Báº¯t Ä‘áº§u training... (Dá»± kiáº¿n: {estimated_hours:.2f} giá»)")
    
    model.fit(
        train_dataloader=train_dataloader,
        epochs=EPOCHS,
        warmup_steps=warmup_steps,
        output_path=OUTPUT_PATH,
        use_amp=True, # Mixed Precision giÃºp giáº£m VRAM vÃ  tÄƒng tá»‘c
        show_progress_bar=True
    )
    
    print(f"âœ… Xong! Model lÆ°u táº¡i: {OUTPUT_PATH}")

if __name__ == "__main__":
    train()