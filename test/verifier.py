import psycopg2
import torch
import numpy as np
import os
import re
from underthesea import sent_tokenize
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.nn.functional import softmax
from dotenv import load_dotenv

# Load bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# --- C·∫§U H√åNH ---
DB_CONFIG = {
    "dbname": os.getenv("POSTGRES_DB", "vnexpress_scraper"),
    "user": os.getenv("POSTGRES_USER", "admin"),
    "password": os.getenv("POSTGRES_PASSWORD", "admin"),
    "host": os.getenv("DB_HOST", "localhost"),
    "port": os.getenv("DB_PORT", "5432")
}

# ƒê∆∞·ªùng d·∫´n Model V6 (Hard Negative Specialist)
MODEL_PATH = "my_model_v6"

class AdvancedFactChecker:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üöÄ KH·ªûI ƒê·ªòNG H·ªÜ TH·ªêNG FACT-CHECKING (V6) TR√äN {self.device.upper()}...")

        # 1. RETRIEVER (Bi-Encoder)
        print("   ‚îú‚îÄ [1/2] Loading Retriever (Search Engine)...")
        self.retriever = SentenceTransformer('bkai-foundation-models/vietnamese-bi-encoder', device=self.device)

        # 2. VERIFIER (Cross-Encoder V6)
        print(f"   ‚îú‚îÄ [2/2] Loading Verifier V6 t·ª´ {MODEL_PATH}...")
        try:
            # D√πng Native Transformers ƒë·ªÉ tr√°nh l·ªói Tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
            self.model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
            self.model.to(self.device)
            self.model.eval() # Ch·∫ø ƒë·ªô ƒë√°nh gi√°
            print("      ‚úÖ Model V6 ƒë√£ s·∫µn s√†ng (Native Mode)!")
        except Exception as e:
            print(f"      ‚ùå L·ªñI LOAD MODEL: {e}")
            print("      üí° G·ª£i √Ω: Ki·ªÉm tra folder model xem c√≥ ƒë·ªß file vocab.txt/config.json kh√¥ng?")
            exit()

    def clean_text(self, text):
        """
        V·ªá sinh vƒÉn b·∫£n ƒë·∫ßu v√†o.
        QUAN TR·ªåNG: Thay xu·ªëng d√≤ng (\n) b·∫±ng d·∫•u ch·∫•m (.) ƒë·ªÉ tr√°nh Title d√≠nh v√†o Body.
        """
        if not text: return ""
        
        # 1. √âp ki·ªÉu string
        text = str(text)
        
        # 2. Thay xu·ªëng d√≤ng b·∫±ng d·∫•u ch·∫•m + c√°ch. 
        # (VD: "Ti√™u ƒë·ªÅ\nN·ªôi dung" -> "Ti√™u ƒë·ªÅ. N·ªôi dung")
        text = text.replace('\n', '. ').replace('\r', '. ').replace('\t', ' ')
        
        # 3. X√≥a c√°c k√Ω t·ª± ch·∫•m th·ª´a (VD: .. -> .)
        text = re.sub(r'\.\.+', '.', text)
        
        # 4. X√≥a kho·∫£ng tr·∫Øng th·ª´a
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def extract_claims(self, text):
        """T√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u ƒë∆°n (Claims)"""
        # B∆∞·ªõc 1: Clean text k·ªπ c√†ng
        cleaned_text = self.clean_text(text)
        
        # B∆∞·ªõc 2: T√°ch c√¢u b·∫±ng Underthesea (T√°ch c√¢u ti·∫øng Vi·ªát chu·∫©n nh·∫•t)
        sentences = sent_tokenize(cleaned_text)
        
        # B∆∞·ªõc 3: L·ªçc c√¢u r√°c
        valid_claims = []
        for s in sentences:
            s = s.strip()
            # B·ªè qua c√¢u qu√° ng·∫Øn (d∆∞·ªõi 5 t·ª´) ho·∫∑c r√°c ƒëi·ªÅu h∆∞·ªõng
            if len(s.split()) < 5: continue
            
            valid_claims.append(s)
            
        return valid_claims

    def predict_pair(self, claim, evidence):
        """
        D·ª± ƒëo√°n quan h·ªá gi·ªØa Claim v√† Evidence.
        Output: List x√°c su·∫•t [Fake, Real, NEI]
        """
        # Tokenize (T·ª± ƒë·ªông th√™m <s> v√† </s> ƒë√∫ng chu·∫©n PhoBERT)
        inputs = self.tokenizer(
            claim, 
            evidence, 
            return_tensors='pt', 
            truncation=True, 
            max_length=256
        ).to(self.device)

        # Inference
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = softmax(outputs.logits, dim=1)[0].cpu().numpy()
        
        # Mapping nh√£n model V6: 0: REFUTED, 1: SUPPORTED, 2: NEI
        return probs

    def verify(self, article_text):
        print("\n" + "="*70)
        print("üìù B·∫ÆT ƒê·∫¶U QUY TR√åNH KI·ªÇM CH·ª®NG...")
        
        claims = self.extract_claims(article_text)
        print(f"üîç T√¨m th·∫•y {len(claims)} c√¢u c·∫ßn ki·ªÉm tra (Claims).")
        
        if not claims: 
            return {"status": "NEUTRAL", "explanation": "N·ªôi dung kh√¥ng ƒë·ªß th√¥ng tin.", "details": []}

        # M√£ h√≥a Claims ƒë·ªÉ t√¨m ki·∫øm
        claim_vectors = self.retriever.encode(claims)
        
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        
        results_list = []
        
        for i, raw_claim in enumerate(claims):
            claim = raw_claim # ƒê√£ clean ·ªü b∆∞·ªõc extract
            
            # --- B∆Ø·ªöC 1: RETRIEVAL (T√åM B·∫∞NG CH·ª®NG) ---
            cur.execute("""
                SELECT content, (embedding <=> %s::vector) as distance
                FROM sentence_store
                ORDER BY distance ASC
                LIMIT 3; 
            """, (claim_vectors[i].tolist(),))
            rows = cur.fetchall()
            
            # L·ªçc ng∆∞·ª°ng: Distance < 0.65 (N·ªõi l·ªèng ch√∫t v√¨ V6 ƒë·ªß kh√¥n ƒë·ªÉ l·ªçc NEI)
            candidates = [self.clean_text(r[0]) for r in rows if r[1] < 0.65]
            
            if not candidates:
                print(f"   ‚ö™ Claim: {claim[:40]}... | SKIP (Kh√¥ng t√¨m th·∫•y data g·ªëc)")
                continue

            best_evidence = candidates[0]

            # --- B∆Ø·ªöC 2: VERIFICATION (MODEL V6 QUY·∫æT ƒê·ªäNH) ---
            # Kh√¥ng d√πng Rule If/Else n·ªØa, tin t∆∞·ªüng Model ho√†n to√†n.
            probs = self.predict_pair(claim, best_evidence)
            
            fake_score = probs[0]
            real_score = probs[1]
            nei_score  = probs[2]
            
            idx = np.argmax(probs)
            
            if idx == 0:   
                status = "REFUTED"
                icon = "üõë"
                confidence = fake_score
            elif idx == 1: 
                status = "SUPPORTED"
                icon = "‚úÖ"
                confidence = real_score
            else:                
                status = "NEI"
                icon = "‚ö™"
                confidence = nei_score
            
            print(f"   {icon} Claim: {claim[:40]}... | {status} ({confidence:.1%})")
            if status == "REFUTED":
                print(f"      ‚û• G·ªëc: {best_evidence[:60]}...")

            results_list.append({
                "claim": claim, 
                "status": status, 
                "evidence": best_evidence, 
                "score": float(confidence),
                "probs": probs.tolist() # L∆∞u full ƒë·ªÉ debug
            })
            
        cur.close()
        conn.close()

        # --- B∆Ø·ªöC 3: K·∫æT LU·∫¨N (AGGREGATION) ---
        return self.make_final_decision(results_list)

    def make_final_decision(self, details):
        if not details:
            return {"status": "NEUTRAL", "confidence": 0, "explanation": "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ƒë·ªëi chi·∫øu."}

        refuted_items = [d for d in details if d['status'] == 'REFUTED']
        supported_items = [d for d in details if d['status'] == 'SUPPORTED']
        
        # --- RULE 1: PH√ÅT HI·ªÜN TIN GI·∫¢ (FAKE) ---
        # Ch·ªâ c·∫ßn 1 c√¢u b·ªã REFUTED v·ªõi ƒë·ªô tin c·∫≠y > 85%
        # Model V6 ƒë√£ h·ªçc Hard Negative n√™n ƒëi·ªÉm REFUTED > 0.85 l√† r·∫•t ƒë√°ng tin.
        strong_fakes = [d for d in refuted_items if d['score'] > 0.85]
        
        if strong_fakes:
            top = strong_fakes[0]
            return {
                "status": "FAKE",
                "confidence": top['score'],
                "explanation": f"Ph√°t hi·ªán sai l·ªách nghi√™m tr·ªçng: '{top['claim']}' m√¢u thu·∫´n v·ªõi d·ªØ li·ªáu g·ªëc.",
                "details": details
            }

        # --- RULE 1.5: NGHI V·∫§N (SUSPICIOUS) ---
        # Tr∆∞·ªùng h·ª£p Model th·∫•y ƒëi·ªÉm FAKE cao (>0.5) nh∆∞ng ch∆∞a th·∫Øng tuy·ªát ƒë·ªëi (v√≠ d·ª• NEI cao h∆°n x√≠u)
        # Ho·∫∑c ƒëi·ªÉm FAKE √°p ƒë·∫£o ƒëi·ªÉm REAL (g·∫•p 3 l·∫ßn)
        for d in details:
            p_fake = d['probs'][0]
            p_real = d['probs'][1]
            if p_fake > 0.5 and p_fake > (p_real * 3):
                 return {
                    "status": "FAKE",
                    "confidence": p_fake,
                    "explanation": f"Nghi v·∫•n sai l·ªách s·ªë li·ªáu/th·ªùi gian: '{d['claim']}'.",
                    "details": details
                }

        # --- RULE 2: X√ÅC NH·∫¨N TIN TH·∫¨T (REAL) ---
        # H∆°n 50% c√¢u l√† SUPPORTED v√† KH√îNG c√≥ c√¢u n√†o REFUTED
        if len(supported_items) >= len(details) * 0.5 and not refuted_items:
            avg_score = sum(d['score'] for d in supported_items) / len(supported_items)
            return {
                "status": "REAL",
                "confidence": avg_score,
                "explanation": "N·ªôi dung b√†i vi·∫øt kh·ªõp v·ªõi d·ªØ li·ªáu ƒë√£ x√°c th·ª±c.",
                "details": details
            }

        # --- RULE 3: TRUNG L·∫¨P ---
        return {
            "status": "NEUTRAL",
            "confidence": 0.5,
            "explanation": "Ch∆∞a ƒë·ªß b·∫±ng ch·ª©ng ƒë·ªÉ k·∫øt lu·∫≠n (Th√¥ng tin h·ªón h·ª£p ho·∫∑c Model kh√¥ng ch·∫Øc ch·∫Øn).",
            "details": details
        }

if __name__ == "__main__":
    checker = AdvancedFactChecker()
    
    # Test case: B·∫´y S·ªë Li·ªáu (Hard Negative)
    # Gi·∫£ s·ª≠ trong DB c√≥: "Vi·ªát Nam c√≥ 7 t·ª∑ d√¢n" (V√≠ d·ª• vui)
    # Input sai: "Vi·ªát Nam c√≥ 70 t·ª∑ d√¢n"
    
    text = "Vi·ªát Nam hi·ªán nay c√≥ d√¢n s·ªë kho·∫£ng 70 t·ª∑ ng∆∞·ªùi."
    print(f"\n>>> Input: {text}")
    
    # L∆∞u √Ω: C·∫ßn c√≥ data trong DB m·ªõi ch·∫°y ƒë∆∞·ª£c nh√©
    # result = checker.verify(text)
    # print(f"üëâ K·∫æT QU·∫¢: {result['status']}")